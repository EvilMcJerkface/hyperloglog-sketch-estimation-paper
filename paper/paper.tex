%#############################
%# Copyright 2016 Otmar Ertl #
%#############################

\documentclass[a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{algpseudocode,algorithm,algorithmicx}
\usepackage{float}
\usepackage{url}
\usepackage{booktabs}
\usepackage{array}
\usepackage{paralist}
\usepackage{verbatim}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{csvsimple}
\usepackage{multirow}
\usepackage[capitalise,nameinlink,noabbrev]{cleveref}

% SYMBOLS
\newcommand*{\symDefine}[2]{\newcommand{{#1}}{{#2}}}
\symDefine{\symPrecision}{p}
\symDefine{\symRegRange}{q}
\symDefine{\symRegVal}{k}
\symDefine{\symRegValVariate}{K}
\symDefine{\symNumReg}{m}
\symDefine{\symDataItem}{D}
\symDefine{\symBitRepA}{a}
\symDefine{\symBitRepB}{b}
\symDefine{\symIndexI}{i}
\symDefine{\symIndexK}{k}
\symDefine{\symIndexL}{l}
\symDefine{\symIndexJ}{j}
\symDefine{\symCount}{c}
\symDefine{\symCountVariate}{C}
\symDefine{\symAlpha}{\alpha}
\symDefine{\symBeta}{\beta}
\symDefine{\symA}{a}
\symDefine{\symB}{b}
\symDefine{\symKappa}{\kappa}
\symDefine{\symX}{x}
\symDefine{\symY}{y}
\symDefine{\symZ}{z}
\symDefine{\symPhi}{\varphi}
\symDefine{\symXEstimate}{\hat{\symX}}
\symDefine{\symXNormalized}{\symX'}
\symDefine{\symN}{n}
\symDefine{\symS}{s}
\symDefine{\symInteger}{z}
\symDefine{\symIntegerVariate}{Z}
\symDefine{\symCardinality}{n}
\symDefine{\symCardinalityEstimate}{\hat{\symCardinality}}
\symDefine{\symCardinalityRawEstimate}{\symCardinalityEstimate_\text{raw}}
\symDefine{\symError}{\varepsilon}
\symDefine{\symStopDelta}{\delta}
\symDefine{\symStopEpsilon}{\varepsilon}
\symDefine{\symBigO}{\mathcal{O}}
\symDefine{\symExpectation}{\mathbb{E}}
\symDefine{\symProbability}{P}
\symDefine{\symProbabilityMass}{\rho}
\symDefine{\symRegProbability}{\gamma}
\symDefine{\symLikelihood}{\mathcal{L}}
\symDefine{\symPoissonRate}{\lambda}
\symDefine{\symPoissonRateEstimate}{\hat{\symPoissonRate}}
\symDefine{\symFunc}{f}
\symDefine{\symFuncPrime}{g}
\symDefine{\symHelper}{h}
\symDefine{\symHelperApprox}{\tilde{\symHelper}}
\symDefine{\symHelperW}{w}
\symDefine{\symHelperV}{v}
\symDefine{\symHelperU}{u}
\symDefine{\symSetA}{A}
\symDefine{\symSetB}{B}
\symDefine{\symSetS}{S}
\symDefine{\symSetX}{X}
\symDefine{\symSetASuffix}{a}
\symDefine{\symSetBSuffix}{b}
\symDefine{\symSetXSuffix}{x}
\symDefine{\symCountMatrix}{\mathbf{c}}
\symDefine{\symPowerSeriesFunc}{\xi}
\symDefine{\symSmallCorrectionFunc}{\sigma}
\symDefine{\symLargeCorrectionFunc}{\tau}
\symDefine{\symEpsPowerSeriesFunc}{\varepsilon_\symPowerSeriesFunc}

\newcommand{\numformat}[1]{{\num[scientific-notation = true,round-mode = places,round-precision = 3, output-exponent-marker = \ensuremath{\mathrm{e}}]{#1}}}

\newcommand{\numformattwo}[1]{{\num[scientific-notation = fixed,round-mode = places,round-precision = 3]{#1}}}


\title{New cardinality estimation algorithms for HyperLogLog sketches}
\subtitle{Draft version}
\author{Otmar Ertl \\ otmar.ertl@gmail.com}
\begin{document}
\maketitle
\let\thefootnote\relax\footnotetext{This paper together with source code for all presented algorithms and simulations is available at \\ \url{https://github.com/oertl/hyperloglog-sketch-estimation-paper}.}
\begin{abstract}
This paper presents new methods to estimate the cardinalities of multisets recorded by HyperLogLog sketches. A theoretically motivated extension to the original estimator is presented that eliminates the bias for small and large cardinalities. Based on the maximum likelihood principle another unbiased method is derived together with a robust and efficient numerical algorithm to calculate the estimate. The maximum likelihood method is also appropriate to improve cardinality estimates of set intersections compared to the inclusion-exclusion principle. The new methods are demonstrated and verified by extensive simulations.
\end{abstract}

\section{Introduction}
Counting the number of distinct elements in a data stream or large datasets is a common problem in big data processing. Often there are parallel streams  or data is spread over a cluster, which makes this task even more challenging.
In principle, finding the number of distinct elements $\symCardinality$ with a maximum relative error $\symError$  in a data stream requires $\Omega(\symCardinality)$ space \cite{Alon1999}. However, probabilistic algorithms that achieve the requested accuracy only with high probability are able to drastically reduce space requirements. Many different probabilistic algorithms have been developed over the past two decades \cite{Metwally2008,Ting2014}. An algorithm with an optimal space complexity of $\Omega(\symError^{-2}+\log \symCardinality)$ \cite{Alon1999, Indyk2003} was finally presented \cite{Kane2010}. This algorithm, however, is not very efficient in practice \cite{Ting2014}.

Currently, the most memory efficient algorithm that also works for distributed setups is the near-optimal HyperLogLog algorithm \cite{Flajolet2007} with space complexity $\Omega(\symError^{-2} \log\log\symCardinality +\log \symCardinality)$. The originally proposed estimation method has some problems to guarantee the same estimation error over the entire range of cardinalities. It was proposed to correct the estimate by empirical means \cite{Heule2013,Rhodes2015,Sanfilippo2014}. 

In case the data is not distributed and results do not need to be aggregated further, there are even more efficient estimation algorithms available. On the one hand there is the self-learning bitmap \cite{Chen2011, Chen2015}, and on the other hand there is the HyperLogLog algorithm extended by a historic inverse probability estimator that is continuously updated together with the HyperLogLog sketch \cite{Ting2014}. Both achieve the same estimation error using less space. However, the estimated cardinality depends on the insertion order of elements and hence cannot be used in a distributed environment. 

%In contrast, HyperLogLog sketches (without historic inverse probability estimator) built from different sets of elements can be merged and the result is identical to a HyperLogLog sketch obtained by inserting all elements of the union set. Therefore, the HyperLogLog sketch inherently supports cardinality estimation of unions. However, there are also applications which require the cardinality estimation of intersections.

\section{HyperLogLog data structure}
The HyperLogLog algorithm uses a sketching data structure that consists of $\symNumReg$ registers. For performance reasons the number of registers $\symNumReg$ is chosen to be a power of 2, $\symNumReg = 2^\symPrecision$. $\symPrecision$ is the precision that directly influences the relative error which scales like $1/\sqrt{\symNumReg}$. All registers start with zero initial value. Each element insertion potentially increases the value of one of these registers. The maximum value a register can reach is a natural bound given either by the output size of the used hash algorithm or the space that is reserved for a single register. Common implementations allocate up to 8 bits per register.

\subsection{Data element insertion}
\label{sec:data_element_insertion}
In order to insert a data element into a HyperLogLog data structure a hash value is calculated. The leading $\symPrecision$ bits of the hash value are used to select one of the $2^\symPrecision$ registers. Among the next $\symRegRange$ bits, the position of the first 1-bit is determined which is a value in the range $[1,\symRegRange+1]$. The value $\symRegRange+1$ is used, if all $\symRegRange$ bits are zeros. If the position of the first 1-bit exceeds the current value of the selected register, the register value is replaced. \cref{alg:insert} shows the update procedure for inserting a data element into the HyperLogLog sketch.

The desribed element insertion algorithm makes use of what is known as stochastic averaging \cite{Flajolet1985}. Instead of updating each of all $\symNumReg$ registers using $\symNumReg$ independent hash values, which would be an $\symBigO(\symNumReg)$ operation, only one register is selected and updated, which requires only a single hash function and reduces the complexity to $\symBigO(1)$.

A HyperLogLog sketch can be characterized by a parameter pair $(\symPrecision, \symRegRange)$. The first parameter $\symPrecision$ is the precision and controls the relative error while the second defines the possible value range of a register. A register can take all values starting from 0 to $\symRegRange+1$, inclusively. The sum $\symPrecision+\symRegRange$ corresponds to the number of consumed hash value bits and defines the maximum cardinality that can be tracked. Obviously, if the cardinality reaches values in the order of $2^{\symPrecision+\symRegRange}$, hash collisions will become more apparent and the estimation accuracy will be drastically reduced.

At any time a $(\symPrecision, \symRegRange)$-HyperLogLog sketch with parameters $(\symPrecision, \symRegRange)$ can be compressed into a $(\symPrecision', \symRegRange')$-HyperLogLog data structure, if $\symPrecision'\leq\symPrecision$ and $\symPrecision'+\symRegRange' \leq\symPrecision + \symRegRange$ is satisfied. This transformation is lossless in a sense that the resulting HyperLogLog sketch is the same as if all elements would have been recorded by a $(\symPrecision', \symRegRange')$-HyperLogLog sketch right from the beginning.

\cref{alg:insert} has some properties which are especially useful in distributed environments. First, the insertion order of elements has no influence on the final HyperLogLog sketch. Furthermore, any two HyperLogLog sketches with same parameters $(\symPrecision, \symRegRange)$ representing  sets $\symSetA$ and $\symSetB$ can be easily merged. The resulting register values are simply given by the register-wise maximum values
\begin{equation}
\label{equ:hll_union}
\symRegValVariate^{\symSetA\cup\symSetB}_{\symIndexI} = 
\max
\!
\left(
\symRegValVariate^{\symSetA}_{\symIndexI},
\symRegValVariate^{\symSetB}_{\symIndexI}
\right)
\quad
\text{for $1\leq\symIndexI\leq\symNumReg$}.
\end{equation}
The final HyperLogLog sketch represents the union of $\symSetA$ and $\symSetB$.

A $(\symPrecision, 0)$-HyperLogLog sketch corresponds to a bit array as used by linear counting \cite{Whang1990}. Each register value can be stored by a single bit in this case. Hence, linear counting can be regarded as a special case of the HyperLogLog algorithm for which $\symRegRange = 0$.

\begin{algorithm}
\caption{Insertion of a data element $\symDataItem$ into a HyperLogLog data structure that consists of $\symNumReg=2^\symPrecision$ registers. All registers $\boldsymbol{\symRegValVariate} = (\symRegValVariate_1,\ldots,\symRegValVariate_\symNumReg)$ have zero initial value and remain in the range $[0, \symRegRange+1]$.}
\label{alg:insert}
\begin{algorithmic}
\Procedure {InsertElement}{\symDataItem, $\boldsymbol{\symRegValVariate}$}
\State $\langle \symBitRepA_1, \ldots, \symBitRepA_\symPrecision,\symBitRepB_1,\ldots,\symBitRepB_\symRegRange\rangle_2 \gets$ $(\symPrecision + \symRegRange)$-bit hash value of $\symDataItem$
\State $\symIndexI \gets 1+ \langle \symBitRepA_1, \ldots, \symBitRepA_\symPrecision\rangle_2$
\Comment $\symIndexI\in[1,2^\symPrecision]$
\State $\symRegVal = \min(\{\symS\mid \symS \in [1, \symRegRange]  \wedge  \symBitRepB_\symS = 1\}\cup {\{\symRegRange+1\}} )$
\Comment $\symRegVal\in[1,\symRegRange+1]$
\State $\symRegValVariate_\symIndexI \gets\max(\symRegValVariate_\symIndexI, \symRegVal)$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Joint probability distribution of register values}

Under the assumption of a uniform hash function, the probability that the register values $\boldsymbol{\symRegValVariate} = (\symRegValVariate_1,\ldots,\symRegValVariate_\symNumReg)$ of a HyperLogLog sketch with parameters $\symPrecision$ and $\symRegRange$ are equal to $\boldsymbol{\symRegVal}=(\symRegVal_1,\ldots,\symRegVal_\symNumReg)$ is given by the corresponding probability mass function
\begin{equation}
\label{equ:multinomialProbabilityMass}
\symProbabilityMass(\boldsymbol{\symRegVal}\vert\symCardinality)
=
\sum_{\symCardinality_1+\ldots+\symCardinality_\symNumReg = \symCardinality} \binom{\symCardinality}{\symCardinality_1,\ldots,\symCardinality_\symNumReg}
\frac{1}{\symNumReg^\symCardinality}\prod_{\symIndexI=1}^\symNumReg \symRegProbability(\symRegVal_\symIndexI\vert\symCardinality_\symIndexI)
\end{equation}
where $\symCardinality$ is the cardinality. The $\symCardinality$ distinct elements are distributed over all $\symNumReg$ registers according to a multinomial distribution with equal probabilities. $\symRegProbability(\symRegVal\vert\symCardinality)$ is the probability that the value of a register is equal to $\symRegVal$, after it was selected $\symCardinality$ times by the insertion algorithm
\begin{equation}
\symRegProbability(\symRegVal\vert\symCardinality) 
:=
\begin{cases}
1 & \symCardinality=0 \wedge \symRegVal = 0\\
0 & \symCardinality=0 \wedge 1\leq\symRegVal\leq\symRegRange+1\\
0 & \symCardinality\geq1 \wedge \symRegVal = 0\\
\left(1-\frac{1}{2^\symRegVal}\right)^\symCardinality - \left(1-\frac{1}{2^{\symRegVal-1}}\right)^\symCardinality & \symCardinality \geq 1 \wedge 1\leq\symRegVal\leq\symRegRange\\
1 - \left(1-\frac{1}{2^{\symRegRange}}\right)^\symCardinality & \symCardinality\geq 1 \wedge \symRegVal = \symRegRange +1
\end{cases}.
\end{equation}

The order of register values $\symRegValVariate_1,\ldots,\symRegValVariate_\symNumReg$ is not important for the estimation of the cardinality. More formally, the multiset $\lbrace\symRegValVariate_1,\ldots,\symRegValVariate_\symNumReg\rbrace$ is a sufficient statistic for $\symCardinality$.
Since the values of the multiset are all in the range $[0, \symRegRange+1]$ the multiset can also be represented as $\lbrace\symRegValVariate_1,\ldots,\symRegValVariate_\symNumReg\rbrace = 0^{\symCountVariate_0}1^{\symCountVariate_1}\cdots\symRegRange^{\symCountVariate_{\symRegRange}}(\symRegRange+1)^{\symCountVariate_{\symRegRange+1}}$ where $\symCountVariate_\symRegVal$ is the multiplicity of value $\symRegVal$. As a consequence, the multiplicity vector $\boldsymbol{\symCountVariate} := (\symCountVariate_0,\ldots,\symCountVariate_{\symRegRange+1})$ is also a sufficient statistic for the cardinality. In addition, this vector contains all the information about the HyperLogLog sketch that is required for cardinality estimation. The two HyperLogLog parameters can be obtained by $\symPrecision = \log_2 \left\Vert\boldsymbol{\symCountVariate}\right\Vert_1$ and $\symRegRange=\dim\boldsymbol{\symCountVariate}-2$, respectively.

\subsection{Poisson approximation}
\label{sec:poisson_approximation}
Due to the statistical dependence of the register values, the probability mass function \eqref{equ:multinomialProbabilityMass} is difficult for further analysis. Therefore, a Poisson model can be used \cite{Flajolet2007}, which assumes that the cardinality itself is distributed according to a Poisson distribution
\begin{equation}
\symCardinality \sim \text{Poisson}(\symPoissonRate).
\end{equation}
Under the Poisson model the distribution of the register values is
\begin{align}
\symProbabilityMass(\boldsymbol{\symRegVal}\vert\symPoissonRate) 
&= 
\sum_{\symCardinality=0}^\infty \symProbabilityMass(\boldsymbol{\symRegVal}\vert\symCardinality) e^{-\symPoissonRate}\frac{\symPoissonRate^\symCardinality}{\symCardinality!}
\label{equ:poissonization}
\\
&= 
\sum_{\symCardinality_1=0}^\infty
\cdots
\sum_{\symCardinality_\symNumReg=0}^\infty
\prod_{\symIndexI=1}^\symNumReg
\symRegProbability(\symRegVal_\symIndexI\vert\symCardinality_\symIndexI)e^{-\frac{\symPoissonRate}{\symNumReg}}\frac{\symPoissonRate^{\symCardinality_\symIndexI}}{\symCardinality_\symIndexI!\symNumReg^{\symCardinality_\symIndexI}}
\nonumber\\
&= 
\prod_{\symIndexI=1}^\symNumReg \sum_{\symCardinality=0}^\infty\symRegProbability(\symRegVal_\symIndexI\vert\symCardinality)e^{-\frac{\symPoissonRate}{\symNumReg}}\frac{\symPoissonRate^\symCardinality}{\symCardinality!\symNumReg^\symCardinality}
\nonumber\\
&= 
\prod_{\symRegVal=0}^{\symRegRange+1} \left(
\sum_{\symCardinality=0}^\infty
\symRegProbability(\symRegVal\vert\symCardinality)e^{-\frac{\symPoissonRate}{\symNumReg}}\frac{\symPoissonRate^\symCardinality}{\symCardinality!\symNumReg^\symCardinality}
\right)^{\!\!\symCount_\symRegVal}
\nonumber\\
\label{equ:poisson_pmf}
&=
e^{-\symCount_0\frac{\symPoissonRate}{\symNumReg}}
\left(\prod_{\symRegVal=1}^{\symRegRange}\left(e^{-\frac{\symPoissonRate}{\symNumReg 2^\symRegVal}}\left(1-e^{-\frac{\symPoissonRate}{\symNumReg 2^\symRegVal}}\right)\right)^{\!\symCount_\symRegVal}\right)
\left(1-e^{-\frac{\symPoissonRate}{\symNumReg 2^\symRegRange}}\right)^{\!\symCount_{\symRegRange+1}}.
\end{align}
Here $\symCount_\symRegVal$ denotes the multiplicity of value $\symRegVal$  in the multiset $\lbrace\symRegVal_1,\ldots,\symRegVal_\symNumReg\rbrace$.
The final factorization shows that under the Poisson model the register values $\symRegValVariate_1,\ldots,\symRegValVariate_\symNumReg$ are independent and identically distributed. The probability that a register has a value less than or equal to $\symRegVal$ for a given rate $\symPoissonRate$ is defined by
\begin{equation}
\label{equ:register_value_distribution}
\symProbability(\symRegValVariate \leq \symRegVal\vert\symPoissonRate)
=
\begin{cases}
0 & \symRegVal < 0 \\
e^{-\frac{\symPoissonRate}{\symNumReg 2^\symRegVal}} & 0\leq \symRegVal \leq \symRegRange \\
1 & \symRegVal > \symRegRange
\end{cases}.
\end{equation}

\subsection{Depoissonization}
\label{sec:depoissonization}
Due to the simpler probability mass function, it is easier to find an estimator $\symPoissonRateEstimate = \symPoissonRateEstimate(\boldsymbol{\symRegValVariate})$ for the Poisson rate $\symPoissonRate$ rather than for the cardinality $\symCardinality$ in the fixed-size model \eqref{equ:multinomialProbabilityMass}. Depoissonization \cite{Jacquet1998} finally allows to translate the estimates back to the fixed-size model. Assume we have found an unbiased estimator for the Poisson rate
\begin{equation}
\symExpectation(\symPoissonRateEstimate\vert\symPoissonRate) = \symPoissonRate
\quad
\text{for all $\symPoissonRate\geq 0$}.
\end{equation}
We know from \eqref{equ:poissonization} 
\begin{equation}
\symExpectation(\symPoissonRateEstimate\vert\symPoissonRate) = 
\sum_{\symCardinality=0}^\infty \symExpectation(\symPoissonRateEstimate\vert\symCardinality) e^{-\symPoissonRate}\frac{\symPoissonRate^\symCardinality}{\symCardinality!}
\end{equation}
and therefore
\begin{equation}
\sum_{\symCardinality=0}^\infty \symExpectation(\symPoissonRateEstimate\vert\symCardinality) e^{-\symPoissonRate}\frac{\symPoissonRate^\symCardinality}{\symCardinality!}
=
\symPoissonRate
\end{equation}
holds for all $\symPoissonRate\geq0$. The unique solution of this equation is given by
\begin{equation}
\symExpectation(\symPoissonRateEstimate\vert\symCardinality) = \symCardinality.
\end{equation}
Hence, the unbiased estimator $\symPoissonRateEstimate$ conditioned on $\symCardinality$ is also an unbiased estimator for $\symCardinality$ which motivates us to use $\symPoissonRateEstimate$ directly as estimator for the cardinality $\symCardinalityEstimate := \symPoissonRateEstimate$. As simulation results will show later, the Poisson approximation works well over the entire cardinality range, even for estimators that are not exactly unbiased.

\section{Original cardinality estimation method}
\label{sec:cardinality_estimation}
The original cardinality estimator \cite{Flajolet2007} is based on the idea that the number of distinct element insertions a register needs to reach the value $\symRegVal$ is proportional to $\symNumReg  2^{\symRegVal}$. Given that, a rough cardinality estimate can be obtained by calculating the average over the values $\lbrace \symNumReg 2^{\symRegValVariate_1},\ldots,\symNumReg2^{\symRegValVariate_\symNumReg}\rbrace$. 

In the history of the HyperLogLog algorithm different averaging techniques have been proposed. First, there was the LogLog algorithm using the geometric mean and the SuperLogLog algorithm that enhanced the estimate by truncating the largest register values before applying the geometric mean \cite{Durand2003}. Finally, the harmonic mean was found to give even better estimates as it is inherently less sensitive to outliers. The result is the so-called raw estimator which is given by
\begin{equation}
\label{equ:raw_estimator}
\symCardinalityRawEstimate
=
\alpha_\symNumReg
\frac{\symNumReg}
{\frac{1}{\symNumReg 2^{\symRegValVariate_1}}+\ldots+\frac{1}{\symNumReg2^{\symRegValVariate_\symNumReg}}}
= 
\frac{\symAlpha_\symNumReg \symNumReg^2}{\sum_{\symIndexI=1}^{\symNumReg}2^{-\symRegValVariate_\symIndexI}}
= 
\frac{\symAlpha_\symNumReg \symNumReg^2}{\sum_{\symRegVal=0}^{\symRegRange+1}\symCountVariate_\symRegVal 2^{-\symRegVal}}.
\end{equation}
Here $\alpha_\symNumReg$ is a bias correction factor which was derived for a given number of registers $\symNumReg$ to be \cite{Flajolet2007}
\begin{equation}
\symAlpha_\symNumReg := \left(
\symNumReg
\int_0^\infty
\left(
\log_2\!\left(
\frac{2+u}{1+u}
\right)
\right)^\symNumReg
du
\right)^{\!\!-1}.
\end{equation}
Numerical approximations of $\symAlpha_\symNumReg$ for various values of $\symNumReg$ have been listed in \cite{Flajolet2007}. These approximations are used in many HyperLogLog implementations. However, since the published constants have been rounded to 4 significant digits, these approximations even introduce some bias for very high precisions $\symPrecision$. For HyperLogLog sketches that are used in practice with 256 or more registers ($\symPrecision\geq 8$), it is completely sufficient to use 
\begin{equation}
\label{equ:alpha_inf}
\symAlpha_\infty := \lim_{\symNumReg\rightarrow\infty} \symAlpha_\symNumReg = \frac{1}{2\log 2} \approx 0.7213475,
\end{equation}
as approximation for $\symAlpha_\symNumReg$ in \eqref{equ:raw_estimator}, because the additional bias is negligible compared to the estimation error.

\cref{fig:raw_estimate} shows the distribution of the relative error for the raw estimator as function of the cardinality. The chart is based on 10,000 randomly generated HyperLogLog sketches. More details of the experimental setup will be explained later in \cref{sec:corrected_raw_estimation_error}. Obviously, the raw estimator is biased for small and large cardinalities where it fails to return accurate estimates. In order to cover the entire range of cardinalities, corrections for small and large cardinalities have been proposed.

As mentioned in \cref{sec:data_element_insertion}, a HyperLogLog sketch with parameters $(\symPrecision, \symRegRange)$ can be mapped to a $(\symPrecision, 0)$-HyperLogLog sketch. Since $\symRegRange=0$ corresponds to linear counting and the reduced HyperLogLog sketch corresponds to a bitset with $\symCountVariate_0$ zeros, the linear counting cardinality estimator \cite{Whang1990} can be used
\begin{equation}
\label{equ:linear_counting_estimator}
\symCardinalityEstimate_\text{small} = \symNumReg \log(\symNumReg/\symCountVariate_0).
\end{equation}
The corresponding relative estimation error as depicted in \cref{fig:small_range_estimate} shows that this estimator is convenient for small cardinalities. It was proposed to use this estimator for small cardinalities as long as $\symCardinalityRawEstimate \leq \frac{5}{2}\symNumReg$ where the factor $\frac{5}{2}$ was empirically determined \cite{Flajolet2007}. 

For large cardinalities in the order of $2^{\symPrecision+\symRegRange}$, for which a lot of registers are already in a saturated state, meaning that they have reached the maximum possible value $\symRegRange + 1$, the raw estimator underestimates the cardinalities. For the 32-bit hash value case $(\symPrecision+\symRegRange=32)$, which was considered in \cite{Flajolet2007}, following correction formula was proposed to take these saturated registers into account
\begin{equation}
\label{equ:large_range_estimate}
\symCardinalityEstimate_\text{large}
=
-2^{32}\log\!\left(1-\symCardinalityRawEstimate/2^{32}\right).
\end{equation}
The original estimation algorithm as presented in \cite{Flajolet2007} including corrections for small and large cardinalities is summarized by \cref{alg:estimate_original}. 
\begin{algorithm}
\caption{Original cardinality estimation algorithm for HyperLogLog sketches that use 32-bit hash values ($\symPrecision+\symRegRange = 32$) for insertion of data items \cite{Flajolet2007}.}
\label{alg:estimate_original}
\begin{algorithmic}
\Function {EstimateCardinality}{$\boldsymbol{\symRegValVariate}$}
\State $\symNumReg \gets \dim \boldsymbol{\symRegValVariate}$
\State $\symCardinalityRawEstimate = \symAlpha_\symNumReg \symNumReg^2\left(\sum_{\symIndexI=1}^\symNumReg 2^{-\symRegValVariate_\symIndexI}\right)^{-1}$
\Comment raw estimate \eqref{equ:raw_estimator}
\If{$\symCardinalityRawEstimate\leq \frac{5}{2}\symNumReg$}
\State $\symCountVariate_0 = \left|\{\symIndexI \vert\symRegValVariate_\symIndexI=0\}\right|$
\If{$\symCountVariate_0\neq0$}
\State \Return $\symNumReg \log(\symNumReg / \symCountVariate_0)$ \Comment small range correction \eqref{equ:linear_counting_estimator}
\Else
\State \Return $\symCardinalityRawEstimate$
\EndIf
\ElsIf{$\symCardinalityRawEstimate\leq \frac{1}{30}2^{32}$}
\State \Return $\symCardinalityRawEstimate$
\Else
\State\Return $-2^{32}\log(1-\symCardinalityRawEstimate/2^{32})$\Comment large range correction \eqref{equ:large_range_estimate}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}
The relative estimation error for the original method is shown in \cref{fig:original_estimate}. Unfortunatley, as can be clearly seen, the ranges where the estimation error is small for $\symCardinalityRawEstimate$ and $\symCardinalityEstimate_\text{small}$ do not overlap. Therefore, the estimation error is much larger near the transition region. To reduce the estimation error for cardinalities close to this region, it was proposed to correct $\symCardinalityRawEstimate$ for bias. Empirically collected bias correction data is either stored as set of interpolation points \cite{Heule2013}, as lookup table \cite{Rhodes2015}, or as best-fitting polynomial \cite{Sanfilippo2014}. However, all these empirical approaches treat the symptom and not the cause.

\begin{figure}
\centering
\includesvg[width=1\textwidth]{raw_estimate}
\caption{The distribution of the relative estimation error over the cardinality for the raw estimator after evaluation of 10,000 randomly generated HyperLogLog data structures with parameters $\symPrecision = 12$ and $\symRegRange=20$.}
\label{fig:raw_estimate}
\end{figure}

\begin{figure}
\centering
\includesvg[width=1\textwidth]{small_range_estimate}
\caption{The distribution of the relative estimation error for the linear counting estimator after evaluation of 10,000 randomly generated bitmaps of size $2^{12}$ which correspond to HyperLogLog sketches with parameters $\symPrecision = 12$ and $\symRegRange=0$.}
\label{fig:small_range_estimate}
\end{figure}

\begin{figure}
\centering
\includesvg[width=1\textwidth]{original_estimate}
\caption{The distribution of the relative estimation error over the cardinality for the original estimation algorithm after evaluation of 10,000 randomly generated HyperLogLog data structures with parameters $\symPrecision = 12$ and $\symRegRange=20$.}
\label{fig:original_estimate}
\end{figure}

The large range correction formula \eqref{equ:large_range_estimate} is not satisfying either as it does not reduce the estimation error. Quite the contrary, it even makes the bias worse. However, instead of underestimating the cardinalities, they are now overestimated. Another indication for the incorrectness of the proposed large range correction is the fact that it is not even defined for all possible states. For instance, consider a $(\symPrecision,\symRegRange)$-HyperLogLog sketch with $\symPrecision+\symRegRange=32$ for which all registers are equal to the maximum possible value $\symRegRange+1$. The raw estimate would be $\symCardinalityRawEstimate = \symAlpha_\symNumReg 2^{33}$, which is greater than $2^{32}$ and outside of the domain of the large range correction formula.

A simple approach to avoid the need of any large range correction is to extend the operating range of the raw estimator to larger cardinalities. This can be easily accomplished by using hash values with more bits $(\symPrecision+\symRegRange>32)$. If $\symRegRange \geq 30$, 5-bit registers are no longer sufficient to represent all possible values in the range $[0, \symRegRange+1]$. If, for example, 64-bit hash values $(\symPrecision+\symRegRange=64)$ are used, as proposed in \cite{Heule2013}, 6 bits per register are needed. Hash value sizes with more than 64 bits are useless in practice, because it is unrealistic to encounter cardinalities of order $2^{64}$ for which the raw estimator would be biased.

\subsection{Derivation of the raw estimator}
\label{sec:derivation_raw_estimator}
In order to better understand why the raw estimator fails for small and large cardinalities, we start with a brief and simple derivation without the restriction to large cardinalities ($\symCardinality\rightarrow\infty$) and without using complex analysis as in \cite{Flajolet2007}.

Let us assume that the register values have following cumulative distribution function
\begin{equation}
\label{equ:assumed_register_val_distribution}
\symProbability(\symRegValVariate \leq \symRegVal\vert\symPoissonRate) = e^{-\frac{\symPoissonRate}{\symNumReg 2^{ \symRegVal}}}.
\end{equation}
For now we ignore that this distribution has infinite support and differs from the register value distribution under the Poisson model \eqref{equ:register_value_distribution}, whose support is limited to the range $[0, \symRegRange+1]$. For a random variable $\symRegValVariate$ obeying \eqref{equ:assumed_register_val_distribution} the expectation of $2^{-\symRegValVariate}$ is given by
\begin{multline}
\label{equ:expectation_power_two}
\symExpectation(2^{-\symRegValVariate})
=
\\
\sum_{\symRegVal = -\infty}^\infty
2^{-\symRegVal}
\left(
e^{-\frac{\symPoissonRate}{\symNumReg 2^{\symRegVal}}}
-e^{-\frac{\symPoissonRate}{\symNumReg 2^{\symRegVal-1}}}
\right)
=
\frac{1}{2}
\sum_{\symRegVal = -\infty}^\infty
2^{\symRegVal}
e^{-\frac{\symPoissonRate}{\symNumReg} 2^{\symRegVal}}
=
\frac{\symAlpha_\infty\,\symNumReg\,\symPowerSeriesFunc\!\left(\log_2\!\left(\symPoissonRate/\symNumReg\right)\right)}{\symPoissonRate},
\end{multline}
where the function 
\begin{equation}
\label{equ:power_series_function}
\symPowerSeriesFunc(\symX):= \log(2) \sum_{\symRegVal = -\infty}^\infty
2^{\symRegVal+\symX}
e^{-2^{\symRegVal+\symX}}
\end{equation}
 is a smooth periodic function with period 1. Numerical evaluations indicate that this function can be bounded by $1 - \symEpsPowerSeriesFunc 
\leq \symPowerSeriesFunc(\symX) \leq 1 + \symEpsPowerSeriesFunc$ with $\symEpsPowerSeriesFunc:=9.885\cdot 10^{-6}$ (see \cref{fig:power_series_func}).

\begin{figure}
\centering
\includesvg[width=0.6\textwidth]{power-series-function-minus-1}
\caption{The deviation of $\symPowerSeriesFunc(\symX)$ from 1.}
\label{fig:power_series_func}
\end{figure}


Let $\symRegValVariate_1,\ldots,\symRegValVariate_\symNumReg$ be a sample taken from \eqref{equ:assumed_register_val_distribution}. For large sample sizes $\symNumReg\rightarrow \infty$ we have asymptotically
\begin{equation}
\symExpectation\!\left(
\frac{1}{2^{-\symRegValVariate_1}+\ldots+2^{-\symRegValVariate_\symNumReg}}
\right)
\underset{\symNumReg\rightarrow \infty}{=}
\frac{1}{\symExpectation
\!\left(2^{-\symRegValVariate_1}+\ldots+2^{-\symRegValVariate_\symNumReg}\right)}
=
\frac{1}{\symNumReg \symExpectation\!\left(2^{-\symRegValVariate}\right)}.
\end{equation}
Together with \eqref{equ:expectation_power_two} we obtain
\begin{equation}
\symPoissonRate
=
\symExpectation\!\left(
\frac{\symAlpha_\infty\,\symNumReg^2\,\symPowerSeriesFunc\!\left(\log_2\!\left( \symPoissonRate/\symNumReg\right)\right)}{2^{-\symRegValVariate_1}+\ldots+2^{-\symRegValVariate_\symNumReg}}
\right)
\quad
\text{for $\symNumReg\rightarrow \infty$}.
\end{equation}
Therefore, the asymptotic relative bias of 
\begin{equation}
\label{equ:rawestimator_2}
\symPoissonRateEstimate 
= 
\frac{\symAlpha_\infty\,\symNumReg^2}{2^{-\symRegValVariate_1}+\ldots+2^{-\symRegValVariate_\symNumReg}}
\end{equation}
is bounded by $\symEpsPowerSeriesFunc$, which makes this statistic a good estimator for the Poisson parameter. It also corresponds to the raw estimator \eqref{equ:raw_estimator}, if the Poisson parameter estimate is used as cardinality estimate (see \cref{sec:depoissonization}).

\subsection{Limitations of the raw estimator}
The raw estimator is based on two prerequisites. First, the number of registers needs to be sufficiently large ($\symNumReg\rightarrow\infty$). And second, the distribution of register values can be described by \eqref{equ:assumed_register_val_distribution}. However, the latter is not true for small and large cardinalities, which is finally the reason for the bias of the raw estimator.

A random variable $\symRegValVariate'$ with cumulative distribution function \eqref{equ:assumed_register_val_distribution} can be transformed into a random variable $\symRegValVariate$ with cumulative distribution function \eqref{equ:register_value_distribution} using
\begin{equation}
\label{equ:dist_transformation}
\symRegValVariate = \min\!\left(\max\!\left(\symRegValVariate', 0\right), \symRegRange+1\right).
\end{equation}
Therefore, the register values $\symRegValVariate_1,\ldots,\symRegValVariate_\symNumReg$ can be seen as the result after applying this transformation to a sample $\symRegValVariate'_1,\ldots,\symRegValVariate'_\symNumReg$ of the distribution described by \eqref{equ:assumed_register_val_distribution}. If all registers values of the HyperLogLog sketch fall into the range $[1,\symRegRange]$, they must  be identical to the values $\symRegValVariate'_1,\ldots,\symRegValVariate'_\symNumReg$. In other words, the observed register values are also a plausible sample of the assumed distribution described by \eqref{equ:assumed_register_val_distribution} in this case. Hence, as long as all or at least most register values are in the range $[1,\symRegRange]$, the approximation of \eqref{equ:register_value_distribution} by \eqref{equ:assumed_register_val_distribution} is valid. This explains why the raw estimator works best for intermediate cardinalities. However, for small and large cardinalities many registers have values equal to 0 or $\symRegRange+1$, respectively, which cannot be described by \eqref{equ:assumed_register_val_distribution} and finally leads to the observed bias.

\subsection{Corrections to the raw estimator}
If we knew the values $\symRegValVariate'_1,\ldots,\symRegValVariate'_\symNumReg$ for which the transformation \eqref{equ:dist_transformation} led to the observed register values $\symRegValVariate_1,\ldots,\symRegValVariate_\symNumReg$, we would be able to estimate $\symPoissonRate$ using
\begin{equation}
\symPoissonRateEstimate 
= 
\frac{\symAlpha_\infty\,\symNumReg^2}{2^{-\symRegValVariate'_1}+\ldots+2^{-\symRegValVariate'_\symNumReg}}.
\end{equation}
As we have already shown, this estimator is approximately unbiased, because all $\symRegValVariate'_\symIndexI$ follow the assumed distribution. It would be even sufficient, if we knew the multiplicity $\symCountVariate'_\symRegVal$
of each $\symRegVal\in\mathbb{Z}$ in $\lbrace\symRegValVariate'_1,\ldots,\symRegValVariate'_\symNumReg\rbrace$, $\symCountVariate'_\symRegVal := \left|\lbrace \symIndexI\vert \symRegVal = \symRegValVariate'_\symIndexI\rbrace\right|$, because the raw estimator can be also written as
\begin{equation}
\label{equ:raw_estimate_with_multiplicities}
\symPoissonRateEstimate 
= 
\frac{\symAlpha_\infty\,\symNumReg^2}{\sum_{\symRegVal=-\infty}^\infty \symCountVariate'_\symRegVal 2^{-\symRegVal}}.
\end{equation}
Due to \eqref{equ:dist_transformation}, the multiplicities $\symCountVariate'_\symRegVal$ and the multiplicities $\symCountVariate_\symRegVal$ for the observed register values have following relationships
\begin{equation}
\label{equ:multiplicity_transformation}
\begin{aligned}
\symCountVariate_0 &= \textstyle\sum_{\symRegVal = -\infty}^{0} \symCountVariate'_\symRegVal, \\
\symCountVariate_\symRegVal &=  \symCountVariate'_\symRegVal, \quad 1\leq\symRegVal\leq\symRegRange,\\
\symCountVariate_{\symRegRange+1} &= \textstyle\sum_{\symRegVal = \symRegRange + 1}^{\infty} \symCountVariate'_\symRegVal.
\end{aligned}
\end{equation}
The idea is now to find estimates $\hat{\symCount}'_\symRegVal$ for all $\symRegVal\in\mathbb{Z}$ and use them as replacements for $\symCountVariate'_\symRegVal$ in \eqref{equ:raw_estimate_with_multiplicities}. For $\symRegVal\in[1,\symRegRange]$ where $\symCountVariate_\symRegVal = \symCountVariate'_\symRegVal$ we can use the trivial estimators 
\begin{equation}
\hat{\symCount}'_\symRegVal := \symCountVariate_\symRegVal,\quad 1\leq\symRegVal\leq\symRegRange.
\end{equation}
To get estimators for $\symRegVal\leq 0$ and $\symRegVal\geq\symRegRange+1$, we consider the expectations
\begin{equation}
\symExpectation(\symCountVariate'_\symRegVal)
=
\symNumReg e^{-\frac{\symPoissonRate}{\symNumReg 2^{\symRegVal}}}\left(1-e^{-\frac{\symPoissonRate}{\symNumReg 2^{\symRegVal}}}\right).
\end{equation}
From \eqref{equ:register_value_distribution} we know that $\symExpectation\!\left(\symCountVariate_0/\symNumReg\right)=
e^{-\frac{\symPoissonRate}{\symNumReg}}$ and $\symExpectation\!\left(1-\symCountVariate_{\symRegRange+1}/\symNumReg\right)=
e^{-\frac{\symPoissonRate}{\symNumReg 2^\symRegRange}}$, and therefore, we can also write
\begin{equation}
\symExpectation(\symCountVariate'_\symRegVal)
=
\symNumReg
\left(\symExpectation\!\left(\symCountVariate_0/\symNumReg\right)\right)^{2^{-\symRegVal}}
\left(1-\left(\symExpectation\!\left(\symCountVariate_0/\symNumReg\right)\right)^{2^{-\symRegVal}}\right)
\end{equation}
and
\begin{equation}
\symExpectation(\symCountVariate'_\symRegVal)
=
\symNumReg
\left(\symExpectation\!\left(1-\symCountVariate_{\symRegRange+1}/\symNumReg\right)\right)^{2^{\symRegRange-\symRegVal}}
\left(1-\left(\symExpectation\!\left(1-\symCountVariate_{\symRegRange+1}/\symNumReg\right)\right)^{2^{\symRegRange-\symRegVal}}\right),
\end{equation}
which motivates us to use
\begin{equation}
\hat{\symCount}'_\symRegVal
=
\symNumReg
\left(\symCountVariate_0/\symNumReg\right)^{2^{-\symRegVal}}
\left(1-\left(\symCountVariate_0/\symNumReg\right)^{2^{-\symRegVal}}\right)
\end{equation}
as estimator for $\symRegVal \leq 0$ and
\begin{equation}
\hat{\symCount}'_\symRegVal
=
\symNumReg
\left(1-\symCountVariate_{\symRegRange+1}/\symNumReg\right)^{2^{\symRegRange-\symRegVal}}
\left(1-\left(1-\symCountVariate_{\symRegRange+1}/\symNumReg\right)^{2^{\symRegRange-\symRegVal}}\right)
\end{equation}
as estimator for $\symRegVal \geq \symRegRange+1$, respectively.

Inserting all these estimators into \eqref{equ:raw_estimate_with_multiplicities} as replacements for $\symCountVariate'_\symRegVal$ finally gives
\begin{equation}
\label{equ:correctedestimator}
\symPoissonRateEstimate 
= 
\frac{\symAlpha_\infty\symNumReg^2}{\sum_{\symRegVal=-\infty}^\infty \hat{\symCount}'_\symRegVal 2^{-\symRegVal}}
=
\frac{\symAlpha_\infty\symNumReg^2}
{
\symNumReg\,\symSmallCorrectionFunc(\symCountVariate_0/\symNumReg) + \sum_{\symRegVal=1}^\symRegRange \symCountVariate_\symRegVal 2^{-\symRegVal} + \symNumReg\, \symLargeCorrectionFunc(1-\symCountVariate_{\symRegRange+1}/\symNumReg) 2^{-\symRegRange}
}
\end{equation}
which we call the corrected raw estimator. Here $\symNumReg\,\symSmallCorrectionFunc(\symCountVariate_0/\symNumReg)$ and $2\symNumReg\,\symLargeCorrectionFunc(1-\symCountVariate_{\symRegRange+1}/\symNumReg)$ are replacements for  $\symCountVariate_0$ and $\symCountVariate_{\symRegRange+1}$ in the raw estimator \eqref{equ:raw_estimator}, respectively. The functions $\symSmallCorrectionFunc$ and $\symLargeCorrectionFunc$ are defined as
\begin{equation}
\symSmallCorrectionFunc(\symX) := 
\symX
+
\sum_{\symRegVal=1}^\infty
\symX^{2^\symRegVal} 2^{\symRegVal-1}
\end{equation}
and
\begin{equation}
\symLargeCorrectionFunc(\symX)
:=
\sum_{\symRegVal=1}^\infty
\symX^{2^{-\symRegVal}}
\left(1-\symX^{2^{-\symRegVal}}\right)
2^{-\symRegVal}.
\end{equation}

Using the identity $\symSmallCorrectionFunc(\symX) + \symLargeCorrectionFunc(\symX) = \symAlpha_\infty\symPowerSeriesFunc(\log_2(\log(1/\symX)))/\log(1/\symX)$, we get for the linear counting case with $\symRegRange=0$
\begin{equation}
\symPoissonRateEstimate 
= 
\frac{\symAlpha_\infty\symNumReg}{
\symSmallCorrectionFunc(\symCountVariate_0/\symNumReg)
+
\symLargeCorrectionFunc(\symCountVariate_0/\symNumReg)
}
=
\frac{\symNumReg\log\!\left(\symNumReg/\symCountVariate_0\right)}{\symPowerSeriesFunc\!\left(\log_2\!\left(\log\!\left(\symNumReg/\symCountVariate_0\right)\right)\right)}
\approx
\symNumReg\log\!\left(\symNumReg/\symCountVariate_0\right),
\end{equation}
which is almost identical to the linear counting estimator used for the small range correction \eqref{equ:linear_counting_estimator}. Here we used again the fact that the function $\symPowerSeriesFunc$ can be well approximated by 1 (see \cref{sec:derivation_raw_estimator}).
\subsection{Corrected raw estimation algorithm}
The corrected raw estimator \eqref{equ:correctedestimator} leads to a new cardinality estimation algorithm for HyperLogLog sketches. \cref{alg:corrected_raw_estimation} demonstrates the numerical evaluation of the estimator. The values of functions $\symSmallCorrectionFunc$ and $\symLargeCorrectionFunc$ can be either calculated on-demand or pre-calculated. The possible value range of $\symCountVariate_0$ and $\symCountVariate_{\symRegRange+1}$ is $[0, \symNumReg]$. Therefore, if performance matters, it is possible to precalculate the function values for all possible arguments and keep them in lookup tables of size $\symNumReg+1$. 

\begin{algorithm}
\caption{Cardinality estimation algorithm based on the corrected raw estimator.}
\label{alg:corrected_raw_estimation}
\begin{algorithmic}
\Function {EstimateCardinality}{$\boldsymbol{\symCountVariate}$}
\State $\symNumReg \gets \left\Vert\boldsymbol{\symCountVariate}\right\Vert_1$
\State $\symZ\gets\symNumReg\cdot\symLargeCorrectionFunc(1 - \symCountVariate_{\symRegRange+1}/\symNumReg)$
\Comment{\parbox[t]{.5\linewidth}{alternatively, take $\symNumReg\cdot\symLargeCorrectionFunc(1 - \symCountVariate_{\symRegRange+1}/\symNumReg)$ from precalculated lookup table}}
\For{$\symRegVal\gets \symRegRange, 1$}
\State $\symZ\gets0.5\cdot\left(\symZ + \symCountVariate_\symRegVal\right)$
\EndFor
\State $\symZ\gets\symZ+\symNumReg\cdot\symSmallCorrectionFunc(\symCountVariate_0/\symNumReg)$
\Comment{\parbox[t]{.5\linewidth}{alternatively, take $\symNumReg\cdot\symSmallCorrectionFunc(\symCountVariate_0/\symNumReg)$ from precalculated lookup table}}
\State \Return$\symAlpha_\infty\symNumReg^2 / \symZ$
\EndFunction
\\
\Function {$\symSmallCorrectionFunc$}{$\symX$}
\Comment $\symX\in[0,1]$
\If{$\symX=1$}
\State\Return$\infty$
\EndIf
\State $\symY\gets1$
\State $\symZ\gets\symX$
\Repeat
\State $\symX\gets\symX\cdot\symX$
\State $\symZ'\gets\symZ$
\State $\symZ\gets\symZ + \symX\cdot\symY$
\State $\symY\gets2\cdot\symY$
\Until{$\symZ=\symZ'$}
\State\Return$\symZ$
\EndFunction
\\
\Function {$\symLargeCorrectionFunc$}{$\symX$}
\Comment $\symX\in[0,1]$
\State $\symY\gets1$
\State $\symZ\gets0$
\Repeat
\State $\symX\gets\sqrt{\symX}$
\State $\symZ'\gets\symZ$
\State $\symY\gets0.5\cdot\symY$
\State $\symZ\gets\symZ + \left(1-\symX\right)\cdot\symX\cdot\symY$
\Until{$\symZ=\symZ'$}
\State\Return$\symZ$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Estimation error}
\label{sec:corrected_raw_estimation_error}
In order to verify the new estimation algorithm, we generated 10,000 HyperLogLog sketches and inserted up to 50 billion unique elements. Assuming a uniform hash function, element hash values can be mocked by random numbers. For the following results we used the Mersenne Twister random number generator with 19,937 bit state size from the C++ standard library.

\cref{fig:raw_corrected_estimation_error_12_20} shows the distribution of the relative error of the estimated cardinality using \cref{alg:corrected_raw_estimation} compared to the true cardinality for $\symPrecision=12$ and $\symRegRange=20$. As the mean shows, the error is unbiased over the entire cardinality range. The new approach is able to accurately estimate cardinalities up to 4 billions ($\approx 2^{\symPrecision+\symRegRange}$) which is about an order of magnitude larger than the operating range upper bound of the raw estimator (\cref{fig:raw_estimate}) or the original method (\cref{fig:original_estimate}).

The corrected raw estimator beats the precision of methods that use bias correction \cite{Heule2013,Rhodes2015,Sanfilippo2014}. \cref{fig:stdev_comparison} shows the standard deviation of the raw estimator after bias correction. At the transition region to small cardinalities, for which all previous methods switch over to the linear counting estimator at some point, the corrected raw estimator has smaller variance. Obviously, the previous approaches cannot do better than given by the minimum of both curves for linear counting and raw estimator. In practice, the standard deviation is even larger, because the choice between both estimators must be made based on an estimate and not on the true cardinality, for which the intersection point of both curves represents the ideal transition point.

\begin{figure}
\centering
\includesvg[width=1\textwidth]{raw_corrected_estimate_12_20}
\caption{Relative error of the corrected raw estimator as a function of the true cardinality for a HyperLogLog sketch with parameters $\symPrecision = 12$ and $\symRegRange=20$.}
\label{fig:raw_corrected_estimation_error_12_20}
\end{figure}

\begin{figure}
\centering
\includesvg[width=1\textwidth]{stdev_comparison}
\caption{Standard deviations of different cardinality estimators as functions of the true cardinality.}
\label{fig:stdev_comparison}
\end{figure}

The new estimation algorithm also works well for other HyperLogLog configurations. First we considered configurations using a 32-bit hash function ($\symPrecision + \symRegRange = 32$). The relative estimation error for precisions $\symPrecision=8$, $\symPrecision=16$, $\symPrecision=22$ are shown in \cref{fig:raw_corrected_estimation_error_8_24}, \cref{fig:raw_corrected_estimation_error_16_16}, and \cref{fig:raw_corrected_estimation_error_22_10}, respectively. As expected, since  $\symPrecision + \symRegRange = 32$ is kept constant, the operating range remains more or less the same, while the relative error decreases with increasing precision. Again, the new algorithm gives essentially unbiased estimates. Only for very large precisions, an oscillating bias becomes apparent (compare \cref{fig:raw_corrected_estimation_error_22_10}), that is caused by approximating the periodic function $\symPowerSeriesFunc$ by a constant (see \cref{sec:derivation_raw_estimator}).

\begin{figure}
\centering
\includesvg[width=1\textwidth]{raw_corrected_estimate_8_24}
\caption{Relative error of the corrected raw estimator as a function of the true cardinality for a HyperLogLog sketch with parameters $\symPrecision = 8$ and $\symRegRange=24$.}
\label{fig:raw_corrected_estimation_error_8_24}
\end{figure}

\begin{figure}
\centering
\includesvg[width=1\textwidth]{raw_corrected_estimate_16_16}
\caption{Relative error of the corrected raw estimator as a function of the true cardinality for a HyperLogLog sketch with parameters $\symPrecision = 16$ and $\symRegRange=16$.}
\label{fig:raw_corrected_estimation_error_16_16}
\end{figure}

\begin{figure}
\centering
\includesvg[width=1\textwidth]{raw_corrected_estimate_22_10}
\caption{Relative error of the corrected raw estimator as a function of the true cardinality for a HyperLogLog sketch with parameters $\symPrecision = 22$ and $\symRegRange=10$.}
\label{fig:raw_corrected_estimation_error_22_10}
\end{figure}

As proposed in \cite{Heule2013}, the operating range can be extended by
replacing the 32-bit hash function by a 64-bit hash function. \cref{fig:raw_corrected_estimation_error_12_52} shows the relative error for such a HyperLogLog configuration with parameters $\symPrecision=12$ and $\symRegRange=52$. The doubled hash value size shifts the maximum trackable cardinality value towards $2^{64}$. As \cref{fig:raw_corrected_estimation_error_12_52} shows, when compared to the 32-bit hash value case given in \cref{fig:raw_corrected_estimation_error_12_20}, the estimation error remains constant over the entire simulated cardinality range up to 50 billions.

\begin{figure}
\centering
\includesvg[width=1\textwidth]{raw_corrected_estimate_12_52}
\caption{Relative error of the corrected raw estimator as a function of the true cardinality for a HyperLogLog sketch with parameters $\symPrecision = 12$ and $\symRegRange=52$.}
\label{fig:raw_corrected_estimation_error_12_52}
\end{figure}

We also evaluated the case $\symPrecision = 12$ and $\symRegRange=14$, which is interesting, because the register values are limited to the range $[0, 15]$. As a consequence, 4 bits are sufficient for representing a single register value. This allows two registers to share a single byte, which is beneficial from a performance perspective. Nevertheless, this configuration still allows the estimation of cardinalities up to 100 millions as shown in \cref{fig:raw_corrected_estimation_error_12_14}, which could be enough for many applications.

\begin{figure}
\centering
\includesvg[width=1\textwidth]{raw_corrected_estimate_12_14}
\caption{Relative error of the corrected raw estimator as a function of the true cardinality for a HyperLogLog sketch with parameters $\symPrecision = 12$ and $\symRegRange=14$.}
\label{fig:raw_corrected_estimation_error_12_14}
\end{figure}

\subsection{Performance}
\label{sec:corrected_raw_estimation_algorithm}
In order to evaluate the performance of the corrected raw estimation algorithm, we investigated the average computation time for estimating the cardinality from a given HyperLogLog sketch. For different cardinalities we loaded precalculated multiplicity vectors of 1000 randomly generated HyperLogLog sketches into main memory. The average computation time was determined by cycling over these multiplicity vectors and passing them as input to the algorithm. For each evaluated cardinality value the average execution time was calculated after 100 cycles which corresponds to 100,000 algorithm executions for each cardinality value. The results for HyperLogLog configurations $\symPrecision=12, \symRegRange=20$ and $\symPrecision=12, \symRegRange=52$ are shown in \cref{fig:corrected_raw_avg_exec_time}. Two variants of \cref{alg:corrected_raw_estimation} have been evaluated for which the functions $\symSmallCorrectionFunc$ and $\symLargeCorrectionFunc$ have been either calculated on-demand or taken from a lookup table. All these benchmarks where carried out on an Intel Core i5-2500K clocking at 3.3\,GHz. 

The results show that the execution times are nearly constant for $\symRegRange=52$. Using a lookup table makes not much difference, because the on-demand calculation for  $\symSmallCorrectionFunc$ is very fast and the calculation of $\symLargeCorrectionFunc$ is rarely needed due to the small probability of saturated registers ($\symCountVariate_{53}>0$) for realistic cardinalities.

For the case $\symRegRange=20$ with precalculated correction values the computation time is again -- as expected -- independent of the cardinality. The faster computation times compared to the $\symRegRange=52$ case can be explained by the much smaller dimension of the multiplicity vector which is equal to $\symRegRange+2$. If the functions $\symSmallCorrectionFunc$ and $\symLargeCorrectionFunc$ are calculated on-demand, the execution times for larger cardinalities are doubled. This comes from the fact that the calculation of $\symLargeCorrectionFunc$ is much more expensive than that of $\symSmallCorrectionFunc$, because it requires more iterations and involves square root evaluations. However, we can imagine that a better numerical approximation of $\symLargeCorrectionFunc$ can be found than that given in \cref{alg:corrected_raw_estimation} and which allows on-demand evaluation without significant extra costs.

The numbers do not yet include the required processing time to extract the multiplicity vector out from the HyperLogLog sketch, which requires a complete scan over all registers and counting the different register values into an array. A theoretical lower bound for this processing time can be derived using the maximum memory bandwidth of the CPU, which is 21\,GB/s for an Intel Core i5-2500K. If we consider a HyperLogLog sketch with precision $\symPrecision=12$ which uses 5 bits per register, the total data size of the HyperLogLog sketch is 2.5\,kB minimum. Consequently, the transfer time from main memory to CPU will be at least 120\,ns. Having this value in mind, the presented numbers for estimating the cardinality from the multiplicity vector are quite satisfying. 

\begin{figure}
\centering
\includesvg[width=1\textwidth]{corrected_raw_avg_exec_time}
\caption{Average computation time as a function of the true cardinality with an Intel Core i5-2500K clocking at 3.3\,GHz when estimating the cardinality from HyperLogLog sketches with parameters $\symPrecision=12$, $\symRegRange=20$ and $\symPrecision=12$, $\symRegRange=52$, respectively. Both cases, $\symSmallCorrectionFunc$ and $\symLargeCorrectionFunc$ precalculated and calculated on-demand have been considered.}
\label{fig:corrected_raw_avg_exec_time}
\end{figure}

\section{Maximum likelihood estimation}
We know from \cref{sec:depoissonization} that any unbiased estimator for the Poisson parameter is also an unbiased estimator for the cardinality. Moreover, we know that under suitable regularity conditions of the probability mass function the maximum likelihood estimator is asymptotically efficient \cite{Casella2002}. This means, if the number of registers $\symNumReg$ is large enough, the maximum likelihood method should give us an unbiased estimator for the cardinality.

We remark that we are not the first applying the maximum likelihood method to HyperLogLog sketches \cite{Clifford2012}, where the case without stochastic averaging (compare \cref{sec:data_element_insertion}) was considered. The register values are statistically independent by nature in this case which 
allows factorization of the joint probability mass function and allows a straightforward application of the maximum likelihood method. 

The more relevant case with stochastic averaging, which is considered in this paper, is more complicated. However, with the help of the Poisson approximation, we are able to still apply the maximum likelihood method and to derive a new robust and efficient cardinality estimation algorithm. Furthermore, we will demonstrate that consequent application of the maximum likelihood method reveals that the cardinality estimate needs to be roughly proportional to the harmonic mean for intermediate cardinality values. The history of the HyperLogLog algorithm shows that the raw estimator \eqref{equ:raw_estimator} was first found after several attempts using the geometric mean \cite{Flajolet2007, Durand2003}.

\subsection{Log-likelihood function}

Using the probability mass function of the Poisson model \eqref{equ:poisson_pmf} the log-likelihood and its derivative are given by
\begin{equation}
\label{equ:log_likelihood_single}
\log \mathcal{\symLikelihood}(\symPoissonRate\vert\boldsymbol{\symRegValVariate}) = 
-\frac{\symPoissonRate}{\symNumReg}\sum_{\symRegVal=0}^{\symRegRange}\frac{\symCountVariate_\symRegVal}{2^\symRegVal}+ 
\sum_{\symRegVal=1}^{\symRegRange}\symCountVariate_\symRegVal \log\!\left(1-e^{-\frac{\symPoissonRate}{\symNumReg 2^\symRegVal}}\right)
+
\symCountVariate_{\symRegRange+1} \log\!\left(1-e^{-\frac{\symPoissonRate}{\symNumReg 2^{\symRegRange}}}\right)
\end{equation}
and
\begin{equation}
\frac{d}{d\symPoissonRate}\log \mathcal{\symLikelihood}(\symPoissonRate\vert\boldsymbol{\symRegValVariate}) 
=
-\frac{1}{\symPoissonRate}\left(
\frac{\symPoissonRate}{\symNumReg}\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}+
\sum_{\symRegVal=1}^\symRegRange \symCountVariate_\symRegVal\frac{\frac{\symPoissonRate}{\symNumReg 2^\symRegVal}}{1-e^{\frac{\symPoissonRate}{\symNumReg 2^\symRegVal}}}
+\symCountVariate_{\symRegRange+1}\frac{\frac{\symPoissonRate}{\symNumReg 2^\symRegRange}}{1-e^{\frac{\symPoissonRate}{\symNumReg 2^\symRegRange}}}
\right).
\end{equation}
As a consequence, the maximum likelihood estimate for the Poisson parameter is given by 
\begin{equation}
\symPoissonRateEstimate = \symNumReg\symXEstimate,
\end{equation}
if $\symXEstimate$ is the root of the function
\begin{equation}
\label{equ:funcdef}
\symFunc(\symX)
:=
\symX\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}+
\sum_{\symRegVal=1}^\symRegRange \symCountVariate_\symRegVal\frac{\frac{\symX}{2^\symRegVal}}{1-e^{\frac{\symX}{2^\symRegVal}}}
+\symCountVariate_{\symRegRange+1}\frac{\frac{\symX}{2^\symRegRange}}{1-e^{\frac{\symX}{2^\symRegRange}}}.
\end{equation}
This function can also be written as
\begin{equation}
\label{equ:func}
\symFunc(\symX)
:=
\symX\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}+
\sum_{\symRegVal=1}^\symRegRange \symCountVariate_\symRegVal\symHelper\!\left(\frac{\symX}{2^\symRegVal}\right)
+
\symCountVariate_{\symRegRange+1}\symHelper\!\left(\frac{\symX}{2^\symRegRange}\right)
-
\left(\symNumReg-\symCountVariate_0\right),
\end{equation}
where the function $\symHelper(\symX)$ is defined as
\begin{equation}
\label{equ:helper}
\symHelper(\symX):=1-\frac{\symX}{e^{\symX}-1}.
\end{equation}
$\symHelper(\symX)$ is strictly increasing and concave as can be seen in \cref{fig:helper_function}. For non-negative values $\symX$ the function ranges from $\symHelper(0)=0$ to $\symHelper(\symX\rightarrow \infty)=1$.
\begin{figure}
\centering
\includesvg[width=0.6\textwidth]{helper}
\caption{The function $\symHelper(\symX)$.}
\label{fig:helper_function}
\end{figure}
Since the function $\symFunc(x)$ is also strictly increasing, it is obvious that there exists a unique root $\symXEstimate$ for which $\symFunc(\symXEstimate)=0$. The function is non-positive at 0 since $\symFunc(0)=\symCountVariate_0-\symNumReg\leq 0$ and, in case $\symCountVariate_{\symRegRange+1}<\symNumReg$ which implies $\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}>0$, the function is at least linearly increasing. $\symCountVariate_{\symRegRange+1}=\symNumReg$ corresponds to the case with all registers equal to the maximum value $\symRegRange+1$, for which the maximum likelihood estimate would be infinite.

It is easy to see that the estimate $\symPoissonRateEstimate$ remains equal or becomes larger, when inserting an element into the HyperLogLog sketch following \cref{alg:insert}. An update potentially changes the multiplicity vector $(\symCountVariate_0,\ldots,\symCountVariate_{\symRegRange+1})$ to $(\symCountVariate_0,\ldots,\symCountVariate_\symIndexI-1,\ldots,\symCountVariate_\symIndexJ+1,\ldots,\symCountVariate_{\symRegRange+1})$ where $\symIndexI < \symIndexJ$. Writing \eqref{equ:func} as 
\begin{multline}
\symFunc(\symX)
:=
\symCountVariate_0 \symX
+
\symCountVariate_1 \left(
\symHelper\!\left(\frac{x}{2^1}\right)
+\frac{\symX}{2^1}-1
\right)
+
\symCountVariate_2 \left(
\symHelper\!\left(\frac{x}{2^2}\right)
+\frac{\symX}{2^2}-1
\right)
+
\ldots
\\
\dots
+
\symCountVariate_\symRegRange
\left(
\symHelper\!\left(\frac{\symX}{2^\symRegRange}\right)
+\frac{\symX}{2^\symRegRange}-1
\right)
+
\symCountVariate_{\symRegRange+1}
\left(
\symHelper\!\left(\frac{\symX}{2^{\symRegRange}}\right)
-1
\right).
\end{multline}
shows that the coefficient of $\symCountVariate_\symIndexI$ is larger than the coefficient of $\symCountVariate_\symIndexJ$ in case $\symIndexI < \symIndexJ$. Keeping $\symX$ fixed during an update decreases $\symFunc(\symX)$. As a consequence, since $\symFunc(\symX)$ is increasing, the new root and hence the estimate must be larger than prior the update.

For the special case $\symRegRange=0$, which corresponds to the already mentioned linear counting algorithm, \eqref{equ:funcdef} can be solved analytically. In this case, the maximum likelihood method under the Poisson model leads directly to the linear counting estimator \eqref{equ:linear_counting_estimator}. Due to this fact we could expect that maximum likelihood estimation under the Poisson model also works well for the more general HyperLogLog case.

\subsection{Inequalities for the maximum likelihood estimate}
In the following lower and upper bounds for $\symXEstimate$ are derived.
Applying Jensen's inequality on $\symHelper$ in \eqref{equ:func} gives an upper bound for $\symFunc(\symX)$:
\begin{equation}
\symFunc(\symX)
\leq
\symX
\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}
+
\left(\symNumReg-\symCountVariate_0\right)\cdot
\symHelper\!\left(
\symX\cdot
\frac{
\sum_{\symRegVal=1}^{\symRegRange}
\frac{\symCountVariate_\symRegVal}{2^\symRegVal}
+
\frac{\symCountVariate_{\symRegRange+1}}{2^\symRegRange}
}
{\symNumReg-\symCountVariate_0}\right)
-
\left(\symNumReg-\symCountVariate_0\right).
\end{equation}
The left-hand side is zero, if $\symXEstimate$ is inserted. Resolution for $\symXEstimate$ finally gives the lower bound
\begin{equation}
\label{equ:strong_lower_bound}
\symXEstimate\geq \frac{\symNumReg-\symCountVariate_0}{\sum_{\symRegVal=1}^{\symRegRange}
\frac{\symCountVariate_\symRegVal}{2^\symRegVal}
+
\frac{\symCountVariate_{\symRegRange+1}}{2^\symRegRange}
}
\log\!\left(
1
+
\frac{\sum_{\symRegVal=1}^{\symRegRange}
\frac{\symCountVariate_\symRegVal}{2^\symRegVal}
+
\frac{\symCountVariate_{\symRegRange+1}}{2^\symRegRange}
}
{
\sum_{\symRegVal=0}^{\symRegRange}
\frac{\symCountVariate_\symRegVal}{2^\symRegVal}
}
\right).
\end{equation}
This bound can be weakened using $\log(1+x) \geq \frac{2x}{x+2}$ for $x\geq0$ which results in
\begin{equation}
\label{equ:weak_lower_bound}
\symXEstimate
\geq
\frac{\symNumReg-\symCountVariate_0}
{\symCountVariate_0+\frac{3}{2}\sum_{\symRegVal=1}^{\symRegRange}\frac{\symCountVariate_\symRegVal}{2^\symRegVal} + \frac{\symCountVariate_{\symRegRange+1}}{2^\symRegRange}}.
\end{equation}
Using the monotonicity of $\symHelper$, the lower bound
\begin{equation}
\symFunc(\symX)
\geq
\symX\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}+
\sum_{\symRegVal=1}^\symRegRange \symCountVariate_\symRegVal\symHelper\!\left(\frac{\symX}{2^{\symRegValVariate'_\text{max}}}\right)
+
\symCountVariate_{\symRegRange+1}\symHelper\!\left(\frac{\symX}{2^{\symRegValVariate'_\text{max}}}\right)
-
\left(\symNumReg-\symCountVariate_0\right)
\end{equation}
can be found, where $\symRegValVariate'_\text{max} := \min(\symRegValVariate_\text{max}, \symRegRange)$ and
$\symRegValVariate_\text{max} := \max\lbrace \symRegVal\vert\symCountVariate_\symRegVal>0\rbrace$.
Again, inserting $\symXEstimate$ and transformation gives
\begin{equation}
\label{equ:strong_upper_bound}
\symXEstimate
\leq
2^{\symRegValVariate'_\text{max}}
\log\!\left(
1+
\frac{\symNumReg-\symCountVariate_0}
{
2^{\symRegValVariate'_\text{max}}
\sum_{\symRegVal=0}^{\symRegRange}
\frac{\symCountVariate_\symRegVal}{2^\symRegVal}
}
\right)
\end{equation}
as upper bound which can be weakened using $\log(1+\symX)\leq \symX$ for $\symX\geq 0$
\begin{equation}
\label{equ:weak_upper_bound}
\symXEstimate
\leq
\frac{\symNumReg-\symCountVariate_0}
{\sum_{\symRegVal=0}^{\symRegRange}
\frac{\symCountVariate_\symRegVal}{2^\symRegVal}}.
\end{equation}
If the HyperLogLog sketch is in the intermediate range, where $\symCountVariate_0=\symCountVariate_{\symRegRange+1}=0$ the bounds \eqref{equ:weak_lower_bound} and \eqref{equ:weak_upper_bound} differ only by a constant factor from the raw estimator \eqref{equ:raw_estimator}. Hence, the maximum likelihood method leads directly to the harmonic mean that is used by the raw estimator.

\subsection{Computation of the maximum likelihood estimate}
\label{sec:comp_ml_estimate}
Since $\symFunc$ is concave and increasing, both, Newton-Raphson iteration and the secant method, will converge to the root, provided that the function is negative for the starting points. In the following we start from the secant method to derive the new cardinality estimation algorithm. Even though the secant method has the disadvantage of slower convergence, a single iteration is simpler to calculate as it does not require the evaluation of the first derivative. An iteration step of the secant method can be written as
\begin{equation}
\symX_{\symIndexI} = 
\symX_{\symIndexI-1} -
\left(\symX_{\symIndexI-1}-\symX_{\symIndexI-2}\right)
\frac{\symFunc(\symX_{\symIndexI-1})}{\symFunc(\symX_{\symIndexI-1}) - \symFunc(\symX_{\symIndexI-2})}
\end{equation}
If we set $\symX_0$ equal to 0, for which  $\symFunc(\symX_0)=-\left(\symNumReg-\symCountVariate_0\right)$, and $\symX_1$ equal to one of the derived lower bounds \eqref{equ:strong_lower_bound} or \eqref{equ:weak_lower_bound}, the sequence $(\symX_0, \symX_1, \symX_2, \ldots)$ is montone increasing. Using the definitions
\begin{equation}
\Delta\symX_\symIndexI := \symX_\symIndexI-\symX_{\symIndexI-1}
\end{equation}
and
\begin{equation}
\label{equ:funcprime}
\symFuncPrime(\symX):=\symFunc(\symX) + \left(\symNumReg-\symCountVariate_0\right)
=\symX\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}+
\sum_{\symRegVal=1}^\symRegRange \symCountVariate_\symRegVal\symHelper\!\left(\frac{\symX}{2^\symRegVal}\right)
+
\symCountVariate_{\symRegRange+1}\symHelper\!\left(\frac{\symX}{2^\symRegRange}\right)
\end{equation}
the iteration scheme can also be written as
\begin{gather}
\label{equ:secant_delta}
\Delta\symX_{\symIndexI} = \Delta\symX_{\symIndexI-1}
\frac{\left(\symNumReg-\symCountVariate_0\right)-\symFuncPrime(\symX_{\symIndexI-1})}{\symFuncPrime(\symX_{\symIndexI-1}) - \symFuncPrime(\symX_{\symIndexI-2})}\\
\symX_{\symIndexI} = \symX_{\symIndexI-1} + \Delta\symX_{\symIndexI}
\end{gather}
The iteration can be stopped, if $\Delta\symX_{\symIndexI} \leq \symStopDelta\cdot \symX_{\symIndexI}$. Since the expected statistical errror for the HyperLogLog data structure scales according to $\frac{1}{\sqrt{\symNumReg}}$ \cite{Flajolet2007}, it makes sense to choose $\symStopDelta = \frac{\symStopEpsilon}{\sqrt{\symNumReg}}$ with some constant $\symStopEpsilon$. For all results that will be presented in \cref{sec:maximum_likelihood_estimation_error} we have used $\symStopEpsilon = 10^{-2}$.

\subsection{Maximum likelihood estimation algorithm}
In order to get a fast cardinality estimation algorithm, it is crucial to minimize the evaluation costs for \eqref{equ:funcprime}. A couple of optimizations allow significant reduction of the computational effort:
\begin{itemize}
\item Only a fraction of all count values $\symCountVariate_\symRegVal$ is non-zero. If we denote $\symRegValVariate_\text{min}:=\min\lbrace \symRegVal\vert\symCountVariate_\symRegVal>0\rbrace$ and $\symRegValVariate_\text{max}:=\max\lbrace \symRegVal\vert\symCountVariate_\symRegVal>0\rbrace$,  it is sufficient to loop over all indices in the range $[\symRegValVariate_\text{min}, \symRegValVariate_\text{max}]$.
\item The sum $\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}$ in \eqref{equ:funcprime} can be precalculated and reused for all function evaluations.
\item Many programming languages allow the efficient multiplication and division by any integral power of 2 using special functions, such as \texttt{ldexp} in C/C++ or \texttt{scalb} in Java.
\item The function $\symHelper(\symX)$ only needs to be evaluated at values $\left\lbrace\frac{\symX}{2^{\symRegValVariate'_\text{max}}},\frac{\symX}{2^{\symRegValVariate'_\text{max}-1}},\ldots,\frac{\symX}{2^{\symRegValVariate_\text{min}}}\right\rbrace$ where $\symRegValVariate'_\text{max} := \min(\symRegValVariate_\text{max}, \symRegRange)$. This series corresponds to a geometric series with ratio 2. A straightforward calculation using \eqref{equ:helper} is very expensive because of the exponential function. However, if we know $\symHelper\!\left(\frac{\symX}{2^{\symRegValVariate'_\text{max}}}\right)$ all other required function values can be easily obtained using the identity
\begin{equation}
\label{equ:helper_recursion1}
\symHelper(4\symX) = \frac{\symX+\symHelper(2\symX)\left(1-\symHelper(2\symX)\right)}{\symX+\left(1-\symHelper(2\symX)\right)}.
\end{equation}
%or
%\begin{equation}
%\label{equ:helper_recursion2}
%\symHelper\!\left(\frac{\symX}{2^{\symRegVal}}\right) = \frac{\frac{\symX}{2^{\symRegVal+2}}+\symHelper\!\left(\frac{\symX}{2^{\symRegVal+1}}\right)\left(1-\symHelper\!\left(\frac{\symX}{2^{\symRegVal+1}}\right)\right)}{\frac{\symX}{2^{\symRegVal+2}}+\left(1-\symHelper\!\left(\frac{\symX}{2^{\symRegVal+1}}\right)\right)}
%\end{equation}
This recursive formula is stable in a sense that the relative error of $\symHelper(4\symX)$ is smaller than that of $\symHelper(2\symX)$ as shown in \cref{app:helper_stable}.

\item If $\symX$ is smaller than 0.5, the function $\symHelper(\symX)$ can be well approximated by a Taylor series around $\symX=0$
\begin{equation}
\symHelper(\symX)
=
\frac{\symX}{2} - \frac{\symX^2}{12} +\frac{\symX^4}{720}-\frac{\symX^6}{30240} + \symBigO(\symX^{8})
\end{equation}
which can be optimized for numerical evaluation using Estrin's scheme and $\symX' := \frac{\symX}{2}$ and $\symX'' := \symX' \symX'$
\begin{equation}
\label{equ:taylor}
\symHelper(\symX)
=
\symX' - \symX''/3 + \left(\symX'' \symX''\right)\left(1/45-\symX''/472.5\right)
+ \symBigO(\symX^{8})
\end{equation}
The smallest argument for which $\symHelper$ needs to be evaluated is $\frac{\symX}{2^{\symRegValVariate'_\text{max}}}$. If 
$\symCountVariate_{\symRegRange+1}=0$, we can find an upper bound for the smallest argument using \eqref{equ:strong_upper_bound}
\begin{equation}
\frac{\symX}{2^{\symRegValVariate'_\text{max}}} 
\leq
\log\!\left(
1+
\frac{\sum_{\symRegVal=0}^{\symRegValVariate'_\text{max}}\symCountVariate_\symRegVal}
{
2^{\symRegValVariate'_\text{max}}
\sum_{\symRegVal=0}^{\symRegValVariate'_\text{max}}
\frac{\symCountVariate_\symRegVal}{2^\symRegVal}
}
\right)
\leq \log 2 \approx 0.693.
\end{equation}
In practice, $\frac{\symX}{2^{\symRegValVariate'_\text{max}}}\leq0.5$ is satisfied most of the time as long as only a few registers are saturated ($\symCountVariate_{\symRegRange+1}\ll \symNumReg$). In case $\frac{\symX}{2^{\symRegValVariate'_\text{max}}} > 0.5$, $\symHelper\!\left(\frac{\symX}{2^{\symKappa}}\right)$ is calculated instead with $\symKappa = 2+\lfloor\log_2(\symX)\rfloor$. By definition, $\frac{\symX}{2^{\symKappa}}\leq 0.5$ which allows using the Taylor series approximation. $\symHelper\!\left(\frac{\symX}{2^{\symRegValVariate'_\text{max}}}\right)$ is finally obtained after $\symKappa -\symRegValVariate'_\text{max}$ iterations using \eqref{equ:helper_recursion1}. As shown in \cref{app:error_approx}, a small approximation error of $\symHelper$ does not have much impact on the error of the maximum likelihood estimate as long as most registers are not yet saturated.
\end{itemize}

Putting all these optimizations together finally gives the new cardinality estimation algorithm as presented in \cref{alg:estimate_ml}. The algorithm requires only elementary operations, except for large cardinalities, where it is worth to take the strong \eqref{equ:strong_lower_bound} instead of the weak lower bound \eqref{equ:weak_lower_bound} as initial value, which requires a single logarithm evaluation. The stronger bound is a much better approximation especially for large cardinalities and substantially reduces the number of required iteration cycles.

\begin{algorithm}
\caption{Maximum likelihood cardinality estimation.}
\ContinuedFloat
\label{alg:estimate_ml}
\begin{algorithmic}
\Function {EstimateCardinality}{$\boldsymbol{\symCountVariate}$}
\State $\symRegRange\gets\dim(\boldsymbol{\symCountVariate})-2$
\State $\symRegValVariate_\text{min} \gets \min\lbrace \symRegVal\vert\symCountVariate_\symRegVal>0\rbrace$
\If{$\symRegValVariate_\text{min} > \symRegRange$}
\State\Return $\infty$
\EndIf
\State $\symRegValVariate'_\text{min} \gets \max(\symRegValVariate_\text{min}, 1)$
\State $\symRegValVariate_\text{max} \gets \max\lbrace \symRegVal\vert\symCountVariate_\symRegVal>0\rbrace$
\State $\symRegValVariate'_\text{max} \gets \min(\symRegValVariate_\text{max}, \symRegRange)$
\State $\symZ \gets 0$
\State $\symNumReg'\gets \symCountVariate_{\symRegRange+1}$
\State $\symY \gets 2^{-\symRegValVariate'_\text{max}}$
\For{$\symRegVal \gets \symRegValVariate'_\text{max},\symRegValVariate'_\text{min}$}
\State $\symZ \gets \symZ + \symCountVariate_\symRegVal\cdot\symY$
\Comment here $\symY = 2^{-\symRegVal}$
\State $\symY \gets 2\symY$
\State $\symNumReg'\gets\symNumReg' + \symCountVariate_\symRegVal$
\EndFor
\State\Comment here $\symZ=\sum_{\symRegVal=1}^{\symRegRange}\frac{\symCountVariate_\symRegVal}{2^\symRegVal}$
\State $\symNumReg \gets \symNumReg' + \symCountVariate_0$ 
\State $\symCount \gets \symCountVariate_{\symRegRange+1}$
\If{$\symRegRange\geq 1$}
\State $\symCount \gets \symCount + \symCountVariate_{\symRegValVariate'_\text{max}}$
\EndIf
\State $\symFuncPrime_\text{prev}\gets 0$
\State $\symA\gets \symZ + \symCountVariate_0$
\Comment $\symA = \sum_{\symRegVal=0}^{\symRegRange}\frac{\symCountVariate_\symRegVal}{2^\symRegVal}$
\State $\symB\gets \symZ + 
\symCountVariate_{\symRegRange+1}\cdot 2^{-\symRegRange}$
\Comment $\symB = \sum_{\symRegVal=1}^{\symRegRange}\frac{\symCountVariate_\symRegVal}{2^\symRegVal} + \frac{\symCountVariate_{\symRegRange+1}}{2^\symRegRange}$
\If{$\symB \leq 1.5\cdot\symA$}
\State $\symX \gets \symNumReg'/(0.5\cdot \symB+\symA)$ \Comment weak lower bound \eqref{equ:weak_lower_bound}
\Else
\State $\symX \gets \symNumReg'/b \cdot \log(1+\symB/\symA)$ \Comment strong lower bound \eqref{equ:strong_lower_bound}
\EndIf
\algstore{myalg}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Maximum likelihood cardinality estimation (continued).}
\begin{algorithmic}
\algrestore{myalg}
\State $\Delta\symX \gets \symX$
\While{$\Delta\symX > \symX\cdot\symError$} \Comment secant method iteration, $\symStopEpsilon = 10^{-2}$ (see \cref{sec:comp_ml_estimate})
\State $\symKappa \gets 2+\lfloor\log_2(\symX)\rfloor$
\State $\symXNormalized \gets
\symX \cdot2^{-\max(\symRegValVariate'_\text{max}, \symKappa)-1}$
\Comment $\symXNormalized \in [0, 0.25]$
\State $\symX''\gets \symXNormalized\cdot\symXNormalized$
\State $\symHelper \gets
\symX' - \symX''/3 + \left(\symX''\cdot \symX''\right)\cdot\left(1/45-\symX''/472.5\right)$
\Comment Taylor approximation \eqref{equ:taylor}
\For{$\symRegVal\gets (\symKappa - 1),\symRegValVariate'_\text{max}$}
\State $\symHelper \gets \frac{\symXNormalized+\symHelper\cdot(1-\symHelper)}{\symXNormalized+(1-\symHelper)}$
\Comment calculate $\symHelper\!\left(\frac{\symX}{2^{\symRegVal}}\right)$, see \eqref{equ:helper_recursion1}, at this point $\symXNormalized = \frac{\symX}{2^{\symRegVal+2}}$
\State $\symXNormalized \gets 2\symXNormalized$
\EndFor
\State $\symFuncPrime \gets \symCount\cdot\symHelper$
\Comment compare \eqref{equ:funcprime}
\For{$\symRegVal\gets(\symRegValVariate'_\text{max}-1),\symRegValVariate'_\text{min}$}
\State $\symHelper \gets \frac{\symXNormalized+\symHelper\cdot(1-\symHelper)}{\symXNormalized+(1-\symHelper)}$
\Comment calculate $\symHelper\!\left(\frac{\symX}{2^{\symRegVal}}\right)$, see \eqref{equ:helper_recursion1}, at this point $\symXNormalized = \frac{\symX}{2^{\symRegVal+2}}$
\State $\symFuncPrime\gets \symFuncPrime + \symCountVariate_\symRegVal\cdot\symHelper$
\State $\symXNormalized \gets 2\symXNormalized$
\EndFor
\State $\symFuncPrime\gets \symFuncPrime + \symX\cdot\symA$
\If{$\symFuncPrime > \symFuncPrime_\text{prev} \wedge \symNumReg' \geq\symFuncPrime$}
\State $\Delta\symX \gets \Delta\symX \cdot \frac{\symNumReg' - \symFuncPrime}{\symFuncPrime - \symFuncPrime_\text{prev}}$
\Comment see \eqref{equ:secant_delta}
\Else
\State $\Delta\symX \gets 0$
\EndIf
\State $\symX \gets \symX + \Delta\symX$
\State $\symFuncPrime_\text{prev}\gets \symFuncPrime$
\EndWhile
\State \Return $\symNumReg\cdot\symX$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Estimation error}
\label{sec:maximum_likelihood_estimation_error}
In order to investigate the estimation error for the maximum likelihood estimation algorithm, we investigated the same HyperLogLog configurations as in \cref{sec:corrected_raw_estimation_error} for the corrected raw estimation algorithm. \cref{fig:max_likelihood_estimation_error_12_20}, \cref{fig:max_likelihood_estimation_error_8_24},  \cref{fig:max_likelihood_estimation_error_16_16}, \cref{fig:max_likelihood_estimation_error_22_10}, \cref{fig:max_likelihood_estimation_error_12_52}, and \cref{fig:max_likelihood_estimation_error_12_14} show very similar results for various HyperLogLog parameters. 

What is different for the maximum likelihood estimation approach is a somewhat smaller median bias with less oscillations around zero for small cardinalities. Furthermore, contrary to the raw estimator which reveals a small oscillating bias for the mean (see \cref{fig:raw_corrected_estimation_error_22_10}), the maximum likelihood estimator seems to be completely unbiased (see \cref{fig:max_likelihood_estimation_error_22_10}).

\begin{figure}
\centering
\includesvg[width=1\textwidth]{max_likelihood_estimate_12_20}
\caption{Relative error of the maximum likelihood estimates as a function of the true cardinality for a HyperLogLog sketch with parameters $\symPrecision = 12$ and $\symRegRange=20$.}
\label{fig:max_likelihood_estimation_error_12_20}
\end{figure}

\begin{figure}
\centering
\includesvg[width=1\textwidth]{max_likelihood_estimate_8_24}
\caption{Relative error of the maximum likelihood estimates as a function of the true cardinality for a HyperLogLog sketch with parameters $\symPrecision = 8$ and $\symRegRange=24$.}
\label{fig:max_likelihood_estimation_error_8_24}
\end{figure}

\begin{figure}
\centering
\includesvg[width=1\textwidth]{max_likelihood_estimate_16_16}
\caption{Relative error of the maximum likelihood estimates as a function of the true cardinality for a HyperLogLog sketch with parameters $\symPrecision = 16$ and $\symRegRange=16$.}
\label{fig:max_likelihood_estimation_error_16_16}
\end{figure}

\begin{figure}
\centering
\includesvg[width=1\textwidth]{max_likelihood_estimate_22_10}
\caption{Relative error of the maximum likelihood estimates as a function of the true cardinality for a HyperLogLog sketch with parameters $\symPrecision = 22$ and $\symRegRange=10$.}
\label{fig:max_likelihood_estimation_error_22_10}
\end{figure}

\begin{figure}
\centering
\includesvg[width=1\textwidth]{max_likelihood_estimate_12_52}
\caption{Relative error of the maximum likelihood estimates as a function of the true cardinality for a HyperLogLog sketch with parameters $\symPrecision = 12$ and $\symRegRange=52$.}
\label{fig:max_likelihood_estimation_error_12_52}
\end{figure}

\begin{figure}
\centering
\includesvg[width=1\textwidth]{max_likelihood_estimate_12_14}
\caption{Relative error of the maximum likelihood estimates as a function of the true cardinality for a HyperLogLog sketch with parameters $\symPrecision = 12$ and $\symRegRange=14$.}
\label{fig:max_likelihood_estimation_error_12_14}
\end{figure}

\subsection{Performance}
We also measured the performance of \cref{alg:estimate_ml} using the same test setup as described in \cref{sec:corrected_raw_estimation_algorithm}. The results for HyperLogLog configurations $\symPrecision=12, \symRegRange=20$ and $\symPrecision=12, \symRegRange=52$ are shown in \cref{fig:avg_exec_time}. The average computation time for the maximum likelihood algorithm shows a different behavior than for the corrected raw estimation algorithm (compare \cref{fig:corrected_raw_avg_exec_time}). As can be seen, the average execution time is larger for most cardinalities, but nevertheless, it never exceeds 700\,ns which is still fast enough for many applications.

\begin{figure}
\centering
\includesvg[width=1\textwidth]{max_likelihood_avg_exec_time}
\caption{Average execution time of the maximum likelihood estimation algorithm as a function of the true cardinality with an Intel Core i5-2500K clocking at 3.3\,GHz for HyperLogLog sketches with parameters $\symPrecision=12$, $\symRegRange=20$ and $\symPrecision=12$, $\symRegRange=52$, respectively.}
\label{fig:avg_exec_time}
\end{figure}

\section{Cardinality estimation of set intersections and differences}

While the union of two sets that are represented by HyperLogLog sketches can be straightforwardly computed \eqref{equ:hll_union}, the computation of cardinalities of other set operations like intersections and differences is more challenging. A common approach to calculate the cardinality of intersections is the inclusion-exclusion principle
\begin{equation}
\label{equ:inclusion-exclusion}
\left\vert\symSetS_1 \cap \symSetS_2\right\vert
=  
\left\vert\symSetS_1\right\vert  
+
\left\vert\symSetS_2\right\vert
-
\left\vert\symSetS_1 \cup \symSetS_2\right\vert.
\end{equation}
Unfortunately, this approach can be very inaccurate. Due to estimation errors of HyperLogLog sketches the result of \eqref{equ:inclusion-exclusion} can even be negative. Motivated by the good results we have obtained for a single HyperLogLog sketch using the maximum likelihood method in combination with the Poisson approximation, we are tempted to also use it for the estimation of set operations. Moreover, it was recently pointed out without special consideration of HyperLogLog sketches, that the application of the maximum likelihood method to the joint likelihood function of two probabilistic data structures which represent the intersection operands gives better intersection cardinality estimates \cite{Ting2016}.

Assume two given  HyperLogLog sketches with register values $\boldsymbol{\symRegValVariate}_1$ and $\boldsymbol{\symRegValVariate}_2$ representing the sets $\symSetS_1$ and $\symSetS_2$, respectively. The goal is to find estimates for the cardinalities of the pair-wise disjoint sets $\symSetX = \symSetS_1\cap\symSetS_2$, $\symSetA = \symSetS_1\setminus\symSetS_2$, and $\symSetB = \symSetS_2\setminus\symSetS_1$. The Poisson approximation allows us to assume that the elements of $\symSetA$ are inserted into the HyperLogLog sketch representing $\symSetS_1$ at rate $\symPoissonRate_\symSetA$. Similarly, the elements of $\symSetB$ are inserted into the HyperLogLog sketch representing $\symSetS_2$ at rate $\symPoissonRate_\symSetB$. Furthermore, we assume that elements of $\symSetX$ are inserted into both HyperLogLog sketches simultaneously at rate $\symPoissonRate_\symSetX$. The goal is now to find estimates, $\symPoissonRateEstimate_\symSetA$, $\symPoissonRateEstimate_\symSetB$, and $\symPoissonRateEstimate_\symSetX$ for the rates which could also be good estimates for the wanted cardinalities of $\symSetA$, $\symSetB$, and $\symSetX$. 

\subsection{Joint log-likelihood funtion}
In order to get maximum-likelihood estimators for $\symPoissonRateEstimate_\symSetA$, $\symPoissonRateEstimate_\symSetB$, and $\symPoissonRateEstimate_\symSetX$ we need to derive the joint probability distribution of two HyperLogLog sketches. Under the Poisson model the register values are independent and identically distributed. Therefore, we first derive the joint probability distribution for a single register that has value $\symRegValVariate_1$ in the first HyperLogLog sketch representing $\symSetS_1$ and value $\symRegValVariate_2$ in the second HyperLogLog sketch representing $\symSetS_2$. 

The HyperLogLog sketch that represents $\symSetS_1$ can be thought to be constructed from two HyperLogLog sketches representing $\symSetA$ and $\symSetX$ and merging both using \eqref{equ:hll_union}. Analogously, the HyperLogLog sketch for $\symSetS_2$ could have been obtained from sketches for $\symSetB$ and $\symSetX$. Let  $\symRegValVariate_\symSetASuffix$, $\symRegValVariate_\symSetBSuffix$, and $\symRegValVariate_\symSetXSuffix$ be the value of the considered register in the HyperLogLog sketch representing $\symSetA$, $\symSetB$, and $\symSetX$, respectively. The corresponding values in sketches for $\symSetS_1$ and $\symSetS_2$ are given by
\begin{equation}
\symRegValVariate_1 = \max\!\left(\symRegValVariate_\symSetASuffix, \symRegValVariate_\symSetXSuffix\right)
,\quad
\symRegValVariate_2 = \max\!\left(\symRegValVariate_\symSetBSuffix, \symRegValVariate_\symSetXSuffix\right).
\end{equation}
Their joint cumulative probability function is given as
\begin{align}
\symProbability\!\left(
\symRegValVariate_1 \leq \symRegVal_1
\wedge
\symRegValVariate_2 \leq \symRegVal_2
\right)
&=
\symProbability\!\left(
\max\!\left(\symRegValVariate_\symSetASuffix, \symRegValVariate_\symSetXSuffix\right) \leq \symRegVal_1
\wedge
\max\!\left(\symRegValVariate_\symSetBSuffix, \symRegValVariate_\symSetXSuffix\right) \leq \symRegVal_2
\right)
\nonumber\\
&=
\symProbability
\!\left(
\symRegValVariate_\symSetASuffix \leq \symRegVal_1
\wedge
\symRegValVariate_\symSetBSuffix \leq \symRegVal_2
\wedge
\symRegValVariate_\symSetXSuffix \leq \min\!\left(\symRegVal_1, \symRegVal_2\right)
\right)
\nonumber\\
&=
\symProbability
\!\left(
\symRegValVariate_\symSetASuffix \leq \symRegVal_1
\right)
\symProbability
\!\left(
\symRegValVariate_\symSetBSuffix \leq \symRegVal_2
\right)
\symProbability
\!\left(
\symRegValVariate_\symSetXSuffix \leq \min\!\left(\symRegVal_1, \symRegVal_2\right)
\right).
\end{align}
Here the last transformation used the independence of $\symRegValVariate_\symSetASuffix$, $\symRegValVariate_\symSetBSuffix$, and $\symRegValVariate_\symSetXSuffix$, because by definition, the sets $\symSetA$, $\symSetB$, and $\symSetX$ are disjoint. Furthermore, under the Poisson model $\symRegValVariate_\symSetASuffix$, $\symRegValVariate_\symSetBSuffix$, and $\symRegValVariate_\symSetXSuffix$ obey 
\eqref{equ:register_value_distribution}. If we take into account that elements are added to $\symSetA$, $\symSetB$, and $\symSetX$ at rates $\symPoissonRate_\symSetXSuffix$, $\symPoissonRate_\symSetASuffix$, and $\symPoissonRate_\symSetBSuffix$, respectively, the probability that a certain register has a value less than or equal to $\symRegVal_1$ in the first HyperLogLog sketch and simultaneously a value less than or equal to $\symRegVal_2$ in the second one can be written as
\begin{equation}
\symProbability(
\symRegValVariate_1 \leq \symRegVal_1
\wedge
\symRegValVariate_2 \leq \symRegVal_2
)
=\begin{cases}
0 & \symRegVal_1 < 0 \vee \symRegVal_2 < 0
\\
e^{
-
\frac{\symPoissonRate_{\symSetASuffix}}{\symNumReg 2^{\symRegVal_1}}
-
\frac{\symPoissonRate_{\symSetBSuffix}}{\symNumReg 2^{\symRegVal_2}}
-
\frac{\symPoissonRate_{\symSetXSuffix}}{\symNumReg 2^{\min(\symRegVal_1, \symRegVal_2)}}
}
& 0\leq\symRegVal_1 \leq \symRegRange \wedge 0\leq\symRegVal_2\leq\symRegRange
\\
e^{
-
\frac{\symPoissonRate_{\symSetBSuffix} + \symPoissonRate_{\symSetXSuffix}}{\symNumReg 2^{\symRegVal_2}}
}
& 0\leq\symRegVal_2 \leq \symRegRange < \symRegVal_1
\\
e^{
-
\frac{\symPoissonRate_{\symSetASuffix} + \symPoissonRate_{\symSetXSuffix}}{\symNumReg 2^{\symRegVal_1}}
}
&  0\leq\symRegVal_1 \leq \symRegRange < \symRegVal_2
\\
1
&
\symRegRange < \symRegVal_1 \wedge \symRegRange < \symRegVal_2
\end{cases}
\end{equation}

The joint probability mass function for both register values can be calculated using
\begin{multline}
\symProbabilityMass(\symRegVal_1,\symRegVal_2)
=
\symProbability(
\symRegValVariate_1 \leq \symRegVal_1
\wedge
\symRegValVariate_2 \leq \symRegVal_2
)
-
\symProbability(
\symRegValVariate_1 \leq \symRegVal_1-1
\wedge
\symRegValVariate_2 \leq \symRegVal_2
)
\\
-\symProbability(
\symRegValVariate_1 \leq \symRegVal_1
\wedge
\symRegValVariate_2 \leq \symRegVal_2-1
)
+\symProbability(
\symRegValVariate_1 \leq \symRegVal_1-1
\wedge
\symRegValVariate_2 \leq \symRegVal_2-1
),
\end{multline}
which finally gives
\begin{multline}
\label{equ:register_value_joint_pmf}
\symProbabilityMass(\symRegVal_1,\symRegVal_2)
=
\\
\begin{cases}
e^{-\frac{\symPoissonRate_\symSetASuffix + \symPoissonRate_\symSetXSuffix}
{\symNumReg }
-\frac{\symPoissonRate_\symSetBSuffix}{\symNumReg 2^{\symRegVal_2}}
}
\left(
1-
e^{
-\frac{\symPoissonRate_\symSetBSuffix}{\symNumReg 2^{\symRegVal_2}}
}
\right)
&
0 = \symRegVal_1 < \symRegVal_2 \leq \symRegRange
\\
e^{-\frac{\symPoissonRate_\symSetASuffix + \symPoissonRate_\symSetXSuffix}
{\symNumReg}
}
\left(
1-
e^{
-\frac{\symPoissonRate_\symSetBSuffix}{\symNumReg 2^{\symRegRange}}
}
\right)
&
0 = \symRegVal_1 < \symRegVal_2 = \symRegRange + 1
\\
e^{-\frac{\symPoissonRate_\symSetASuffix + \symPoissonRate_\symSetXSuffix}
{\symNumReg 2^{\symRegVal_1}}
-\frac{\symPoissonRate_\symSetBSuffix}{\symNumReg 2^{\symRegVal_2}}
}
\left(
1-
e^{-\frac{\symPoissonRate_\symSetASuffix + \symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\symRegVal_1}}
}
\right)
\left(
1-
e^{
-\frac{\symPoissonRate_\symSetBSuffix}{\symNumReg 2^{\symRegVal_2}}
}
\right)
&
1 \leq \symRegVal_1 < \symRegVal_2 \leq \symRegRange
\\
e^{-\frac{\symPoissonRate_\symSetASuffix + \symPoissonRate_\symSetXSuffix}
{\symNumReg 2^{\symRegVal_1}}
}
\left(
1-
e^{-\frac{\symPoissonRate_\symSetASuffix + \symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\symRegVal_1}}
}
\right)
\left(
1-
e^{
-\frac{\symPoissonRate_\symSetBSuffix}{\symNumReg 2^{\symRegRange}}
}
\right)
&
1 \leq \symRegVal_1 < \symRegVal_2 = \symRegRange + 1
\\
e^{-\frac{\symPoissonRate_\symSetBSuffix + \symPoissonRate_\symSetXSuffix}
{\symNumReg }
-\frac{\symPoissonRate_\symSetASuffix}{\symNumReg 2^{\symRegVal_1}}
}
\left(
1-
e^{
-\frac{\symPoissonRate_\symSetASuffix}{\symNumReg 2^{\symRegVal_1}}
}
\right)
&
0 = \symRegVal_2 < \symRegVal_1 \leq \symRegRange
\\
e^{-\frac{\symPoissonRate_\symSetBSuffix + \symPoissonRate_\symSetXSuffix}
{\symNumReg}
}
\left(
1-
e^{
-\frac{\symPoissonRate_\symSetASuffix}{\symNumReg 2^{\symRegRange}}
}
\right)
&
0 = \symRegVal_2 < \symRegVal_1 = \symRegRange + 1
\\
e^{-\frac{\symPoissonRate_\symSetBSuffix + \symPoissonRate_\symSetXSuffix}
{\symNumReg 2^{\symRegVal_2}}
-\frac{\symPoissonRate_\symSetASuffix}{\symNumReg 2^{\symRegVal_1}}
}
\left(
1-
e^{-\frac{\symPoissonRate_\symSetBSuffix + \symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\symRegVal_2}}
}
\right)
\left(
1-
e^{
-\frac{\symPoissonRate_\symSetASuffix}{\symNumReg 2^{\symRegVal_1}}
}
\right)
&
1 \leq \symRegVal_2 < \symRegVal_1 \leq \symRegRange
\\
e^{-\frac{\symPoissonRate_\symSetBSuffix + \symPoissonRate_\symSetXSuffix}
{\symNumReg 2^{\symRegVal_2}}
}
\left(
1-
e^{-\frac{\symPoissonRate_\symSetBSuffix + \symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\symRegVal_2}}
}
\right)
\left(
1-
e^{
-\frac{\symPoissonRate_\symSetASuffix}{\symNumReg 2^{\symRegRange}}
}
\right)
&
1 \leq \symRegVal_2 < \symRegVal_1 = \symRegRange + 1
\\
e^{-\frac{\symPoissonRate_\symSetASuffix + \symPoissonRate_\symSetBSuffix + \symPoissonRate_\symSetXSuffix}
{\symNumReg}
}
&
0 = \symRegVal_1 = \symRegVal_2
\\
e^{-\frac{\symPoissonRate_\symSetASuffix + \symPoissonRate_\symSetBSuffix + \symPoissonRate_\symSetXSuffix}
{\symNumReg 2^\symRegVal}
}
\left(
1
-
e^{-\frac{\symPoissonRate_\symSetASuffix +  \symPoissonRate_\symSetXSuffix}
{\symNumReg 2^\symRegVal}
}
-
e^{-\frac{\symPoissonRate_\symSetBSuffix + \symPoissonRate_\symSetXSuffix}
{\symNumReg 2^\symRegVal}
}
+
e^{-\frac{\symPoissonRate_\symSetASuffix + \symPoissonRate_\symSetBSuffix + \symPoissonRate_\symSetXSuffix}
{\symNumReg 2^\symRegVal}
}
\right)
&
1 \leq \symRegVal_1 = \symRegVal_2 = \symRegVal\leq \symRegRange
\\
1
-
e^{-\frac{\symPoissonRate_\symSetASuffix +  \symPoissonRate_\symSetXSuffix}
{\symNumReg 2^\symRegRange}
}
-
e^{-\frac{\symPoissonRate_\symSetBSuffix + \symPoissonRate_\symSetXSuffix}
{\symNumReg 2^\symRegRange}
}
+
e^{-\frac{\symPoissonRate_\symSetASuffix + \symPoissonRate_\symSetBSuffix + \symPoissonRate_\symSetXSuffix}
{\symNumReg 2^\symRegRange}
}
&
\symRegVal_1 = \symRegVal_2 = \symRegRange + 1
\end{cases}
\end{multline}

The logarithm of the joint probability mass function can be written using Iverson bracket notation ($\left[\text{true}\right]:=1$, $\left[\text{false}\right]:=0$) as
\begin{align}
\label{equ:joint_log_pmf_single_register}
\log(\symProbabilityMass(\symRegVal_1,\symRegVal_2))
=&
-\frac{\symPoissonRate_\symSetASuffix}{\symNumReg 2^{\symRegVal_1}}
\left[\symRegVal_1\leq\symRegRange\right]
-
\frac{\symPoissonRate_\symSetBSuffix}{\symNumReg 2^{\symRegVal_2}}
\left[\symRegVal_2\leq\symRegRange\right]
-
\frac{\symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\min\left(\symRegVal_1,\symRegVal_2\right)}}
\left[\symRegVal_1\leq\symRegRange\vee\symRegVal_2\leq\symRegRange\right]
\nonumber\\
&
+
\log\!\left(1-e^{-\frac{\symPoissonRate_\symSetASuffix+\symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\symRegVal_1}}}\right)
\left[1\leq\symRegVal_1<\symRegVal_2\right]
\nonumber\\
&
+
\log\!\left(1-e^{-\frac{\symPoissonRate_\symSetASuffix}{\symNumReg 2^{\min\left(\symRegVal_1, \symRegRange\right)}}}\right)
\left[\symRegVal_2<\symRegVal_1\right]
\nonumber\\
&
+
\log\!\left(1-e^{-\frac{\symPoissonRate_\symSetBSuffix+\symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\symRegVal_2}}}\right)
\left[1\leq\symRegVal_2<\symRegVal_1\right]
\nonumber\\
&
+
\log\!\left(1-e^{-\frac{\symPoissonRate_\symSetBSuffix}{\symNumReg 2^{\min\left(\symRegVal_2, \symRegRange\right)}}}\right)
\left[\symRegVal_1<\symRegVal_2\right]
\nonumber\\
&
+
\log\!\left(
1
-e^{-\frac{\symPoissonRate_\symSetASuffix+\symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\min\left(\symRegVal_1, \symRegRange\right)}}}
-
e^{-\frac{\symPoissonRate_\symSetBSuffix+\symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\min\left(\symRegVal_1, \symRegRange\right)}}}
+
e^{-\frac{\symPoissonRate_\symSetASuffix+\symPoissonRate_\symSetBSuffix+\symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\min\left(\symRegVal_1, \symRegRange\right)}}}
\right)
\left[1\leq\symRegVal_1=\symRegVal_2\right].
\end{align}

Since the values for different registers are independent under the Poisson model, we are now able to write the joint probability mass function for all registers in both HyperLogLog sketches
\begin{equation}
\symProbabilityMass(\boldsymbol{\symRegVal}_1,\boldsymbol{\symRegVal}_2)
=
\prod_{\symIndexI = 1}^{\symNumReg}
\prod_{\symIndexJ = 1}^{\symNumReg}
\symProbabilityMass(\symRegVal_{1\symIndexI},\symRegVal_{2\symIndexJ}).
\end{equation}

In order to get the maximum likelihood estimates $\symPoissonRateEstimate_\symSetASuffix$,
 $\symPoissonRateEstimate_\symSetBSuffix$, and  $\symPoissonRateEstimate_\symSetXSuffix$ we need to maximize the log-likelihood function given by
\begin{equation}
\log \symLikelihood(
\symPoissonRate_\symSetASuffix,
\symPoissonRate_\symSetBSuffix,
\symPoissonRate_\symSetXSuffix
\vert
\boldsymbol{\symRegValVariate}_1,
\boldsymbol{\symRegValVariate}_2
)
=
\sum_{\symIndexI = 1}^{\symNumReg}
\sum_{\symIndexJ = 1}^{\symNumReg}
\log(\symProbabilityMass(\symRegValVariate_{1\symIndexI},\symRegValVariate_{2\symIndexJ}))
\end{equation}
Insertion of \eqref{equ:joint_log_pmf_single_register} results in the generalization of \eqref{equ:log_likelihood_single} for two HyperLogLog sketches
\begin{align}
\label{equ:log_likelihood_pair}
\log \symLikelihood(
\symPoissonRate_\symSetASuffix,
\symPoissonRate_\symSetBSuffix,
\symPoissonRate_\symSetXSuffix
\vert
\boldsymbol{\symRegValVariate}_1,
\boldsymbol{\symRegValVariate}_2
)
=
&
-
\frac{\symPoissonRate_\symSetASuffix}{\symNumReg}
\sum_{\symRegVal=0}^{\symRegRange}
\frac{
  \symCountVariate^{<}_{1\symRegVal}+
  \symCountVariate^{=}_{\symRegVal}+
  \symCountVariate^{>}_{1\symRegVal}
}{2^{\symRegVal}}
-
\frac{\symPoissonRate_\symSetBSuffix}{\symNumReg}
\sum_{\symRegVal=0}^{\symRegRange}
\frac{
  \symCountVariate^{<}_{2\symRegVal}+
  \symCountVariate^{=}_{\symRegVal}+
  \symCountVariate^{>}_{2\symRegVal}
}{2^{\symRegVal}}
\nonumber\\
&
-
\frac{\symPoissonRate_\symSetXSuffix}{\symNumReg}
\sum_{\symRegVal=0}^{\symRegRange}
\frac{
  \symCountVariate^{<}_{1\symRegVal}+
  \symCountVariate^{=}_{\symRegVal}+
  \symCountVariate^{<}_{2\symRegVal}
}{2^{\symRegVal}}
\nonumber\\
&
+
\sum_{\symRegVal=1}^{\symRegRange}
\log\!\left(1-e^{-\frac{\symPoissonRate_\symSetASuffix+\symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\symRegVal}}}\right)
\symCountVariate^{<}_{1\symRegVal}
+
\log\!\left(1-e^{-\frac{\symPoissonRate_\symSetBSuffix+\symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\symRegVal}}}\right)
\symCountVariate^{<}_{2\symRegVal}
\nonumber\\
&
+
\sum_{\symRegVal=1}^{\symRegRange+1}
\log\!\left(1-e^{-\frac{\symPoissonRate_\symSetASuffix}{\symNumReg 2^{\min\left(\symRegVal,\symRegRange\right)}}}\right)
\symCountVariate^{>}_{1\symRegVal}
+
\log\!\left(1-e^{-\frac{\symPoissonRate_\symSetBSuffix}{\symNumReg 2^{\min\left(\symRegVal,\symRegRange\right)}}}\right)
\symCountVariate^{>}_{2\symRegVal}
\nonumber\\
&
+
\sum_{\symRegVal=1}^{\symRegRange+1}
\log\!\left(
1
-e^{-\frac{\symPoissonRate_\symSetASuffix+\symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\min\left(\symRegVal,\symRegRange\right)}}}
-
e^{-\frac{\symPoissonRate_\symSetBSuffix+\symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\min\left(\symRegVal,\symRegRange\right)}}}
+
e^{-\frac{\symPoissonRate_\symSetASuffix+\symPoissonRate_\symSetBSuffix+\symPoissonRate_\symSetXSuffix}{\symNumReg 2^{\min\left(\symRegVal,\symRegRange\right)}}}
\right)
\symCountVariate^{=}_{\symRegVal},
\end{align}
where the constants $\symCountVariate^{>}_{1\symRegVal}$,
$\symCountVariate^{<}_{1\symRegVal}$,
$\symCountVariate^{>}_{2\symRegVal}$,
$\symCountVariate^{<}_{2\symRegVal}$,
and $\symCountVariate^{=}_{\symRegVal}$ are defined as
\begin{equation}
\label{equ:sufficient_joint_statistic}
\begin{aligned}
\symCountVariate^{>}_{1\symRegVal}&:=\left|\lbrace\symIndexI\vert\symRegVal=\symRegValVariate_{1\symIndexI}>
\symRegValVariate_{2\symIndexI}\rbrace\right|,
\\
\symCountVariate^{<}_{1\symRegVal}&:=\left|\lbrace\symIndexI\vert\symRegVal=\symRegValVariate_{1\symIndexI}<
\symRegValVariate_{2\symIndexI}\rbrace\right|,
\\
\symCountVariate^{>}_{2\symRegVal}&:=\left|\lbrace\symIndexI\vert\symRegVal=\symRegValVariate_{2\symIndexI}>
\symRegValVariate_{1\symIndexI}\rbrace\right|,
\\
\symCountVariate^{<}_{2\symRegVal}&:=\left|\lbrace\symIndexI\vert\symRegVal=\symRegValVariate_{2\symIndexI}<
\symRegValVariate_{1\symIndexI}\rbrace\right|,
\\
\symCountVariate^{=}_{\symRegVal}&:=\left|\lbrace\symIndexI\vert\symRegVal=\symRegValVariate_{1\symIndexI}=
\symRegValVariate_{2\symIndexI}\rbrace\right|.
\end{aligned}
\end{equation}
These $5\symNumReg$ values are sufficient for estimating $\symPoissonRate_\symSetASuffix$, $\symPoissonRate_\symSetBSuffix$, and $\symPoissonRate_\symSetXSuffix$. Actually, the set of these constants can be further reduced, because $\symCountVariate^{>}_{10}=\symCountVariate^{>}_{20}=
\symCountVariate^{<}_{1(\symRegRange+1)}=\symCountVariate^{<}_{2(\symRegRange+1)}=0$
always holds.

The log-likelihood function \eqref{equ:log_likelihood_pair} does not always have a strict global maximum point. For example, if all register values of the first HyperLogLog sketch are larger than the corresponding values in the second HyperLogLog sketch, $\symCountVariate_{1\symRegVal}^{<}
=
\symCountVariate_{\symRegVal}^{=}
=
\symCountVariate_{2\symRegVal}^{>}
=
0$ for all $\symRegVal$, the function can be rewritten as sum of two functions, one dependent on $\symPoissonRate_\symSetASuffix$ and the other dependent on $\symPoissonRate_\symSetBSuffix+\symPoissonRate_\symSetXSuffix$. 
The maximum is obtained, if $\symPoissonRate_\symSetASuffix = \symPoissonRateEstimate_1$ and $\symPoissonRate_\symSetBSuffix+\symPoissonRate_\symSetXSuffix = \symPoissonRateEstimate_2$. Here $\symPoissonRateEstimate_1$ and $\symPoissonRateEstimate_2$ are the maximum likelihood cardinality estimates for the first and second HyperLogLog sketch, respectively. This means that the maximum-likelihood makes no clear statement about the intersection size in this case. The estimate for $\symPoissonRate_\symX$ could be anything between 0 and $\symPoissonRateEstimate_2$. For comparison, the inclusion-exclusion approach would give $\symPoissonRateEstimate_2$ as estimate. This result is questionable, because there is no evidence at all, that the sets $\symSetS_1$ and $\symSetS_2$ really have common elements.

If it is given that the two HyperLogLog sketches represent disjoint sets, we know in advance that $\symPoissonRate_\symSetXSuffix=0$. In this case \eqref{equ:log_likelihood_pair} can be splitted into the sum of two unary functions with parameters $\symPoissonRate_\symSetASuffix$ and $\symPoissonRate_\symSetBSuffix$, respectively. As expected, these two functions are equivalent to the individual log-likelihood functions \eqref{equ:log_likelihood_single} of both HyperLogLog sketches.

The inclusion-exclusion method does not use all the information given by the sufficient statistic \eqref{equ:sufficient_joint_statistic}, because the estimates are functions of the three vectors $(\boldsymbol\symCountVariate^{<}_{1} + \boldsymbol\symCountVariate^{=} + \boldsymbol\symCountVariate^{>}_{1})$, 
$(\boldsymbol\symCountVariate^{<}_{2} + \boldsymbol\symCountVariate^{=} + \boldsymbol\symCountVariate^{>}_{2})$, and 
$(\boldsymbol\symCountVariate^{>}_{1} + \boldsymbol\symCountVariate^{=} + \boldsymbol\symCountVariate^{>}_{2})$. In contrast, the maximum-likelihood method uses all values of the sufficient statistic \eqref{equ:sufficient_joint_statistic}.

\subsection{Computation of the maximum likelihood estimates}
The maximum likelihood estimates can be obtained by maximizing \eqref{equ:log_likelihood_pair}. Since the three parameters are all non-negative, this is a constrained optimization problem. In order to get rid of these constraints, we use the transformation $\symPoissonRate=\symNumReg e^{\symPhi}$. This mapping has also the nice property that relative accuracy limits are translated into absolute ones, because $\Delta\symPhi = \Delta\symPoissonRate/\symPoissonRate$. Many optimization algorithm implementations allow the definition of absolute limits rather than relative ones.

The transformed log-likelihood function can be written as
%\begin{multline}
%\symFunc(\symPhi_\symSetASuffix,\symPhi_\symSetBSuffix,\symPhi_\symSetXSuffix):=
%\log \symLikelihood(
%\symNumReg e^{\symPhi_\symSetASuffix},
%\symNumReg e^{\symPhi_\symSetBSuffix},
%\symNumReg e^{\symPhi_\symSetXSuffix}
%\vert
%\boldsymbol{\symRegValVariate}_1,
%\boldsymbol{\symRegValVariate}_2
%)=
%\\
%\begin{aligned}
%&-
%\sum_{\symRegVal=0}^{\symRegRange}
%\left(
%  \symCountVariate^{<}_{1\symRegVal}+
%  \symCountVariate^{=}_{\symRegVal}+
%  \symCountVariate^{>}_{1\symRegVal}
%\right)
%\frac{e^{\symPhi_\symSetASuffix}}{2^{\symRegVal}}
%+
%\left(
%  \symCountVariate^{<}_{2\symRegVal}+
%  \symCountVariate^{=}_{\symRegVal}+
%  \symCountVariate^{>}_{2\symRegVal}
%\right)
%\frac{e^{\symPhi_\symSetBSuffix}}{2^{\symRegVal}}
%+
%\left(
%  \symCountVariate^{<}_{1\symRegVal}+
%  \symCountVariate^{=}_{\symRegVal}+
%  \symCountVariate^{<}_{2\symRegVal}
%\right)
%\frac{e^{\symPhi_\symSetXSuffix}}{2^{\symRegVal}}
%\\
%&+
%\sum_{\symRegVal=1}^{\symRegRange}
%\symCountVariate^{<}_{1\symRegVal}
%\log\!\left(1-e^{-\frac{e^{\symPhi_\symSetASuffix}
%+e^{\symPhi_\symSetXSuffix}
%}{2^{\symRegVal}}}\right)
%+
%\symCountVariate^{<}_{2\symRegVal}
%\log\!\left(1-e^{-\frac{e^{\symPhi_\symSetBSuffix}
%+
%e^{\symPhi_\symSetXSuffix}
%}{2^{\symRegVal}}}\right)
%\\
%&+
%\sum_{\symRegVal=1}^{\symRegRange+1}
%\symCountVariate^{>}_{1\symRegVal}
%\log\!\left(1-e^{-\frac{e^{\symPhi_\symSetASuffix}
%}{2^{\min\left(\symRegVal,\symRegRange\right)}}}\right)
%+
%\symCountVariate^{>}_{2\symRegVal}
%\log\!\left(1-e^{-\frac{e^{\symPhi_\symSetBSuffix}
%}{2^{\min\left(\symRegVal,\symRegRange\right)}}}\right)
%\\
%&+
%\sum_{\symRegVal=1}^{\symRegRange+1}
%\symCountVariate^{=}_{\symRegVal}
%\log\!\left(
%1
%-e^{-\frac{e^{\symPhi_\symSetASuffix}
%+e^{\symPhi_\symSetXSuffix}
%}{2^{\min\left(\symRegVal,\symRegRange\right)}}}
%-
%e^{-\frac{e^{\symPhi_\symSetBSuffix}
%+e^{\symPhi_\symSetXSuffix}
%}{2^{\min\left(\symRegVal,\symRegRange\right)}}}
%+
%e^{-\frac{e^{\symPhi_\symSetASuffix}
%+e^{\symPhi_\symSetBSuffix}
%+e^{\symPhi_\symSetXSuffix}
%}{2^{\min\left(\symRegVal,\symRegRange\right)}}}
%\right).
%\end{aligned}
%\end{multline}

\begin{multline}
\symFunc(\symPhi_\symSetASuffix,\symPhi_\symSetBSuffix,\symPhi_\symSetXSuffix):=
\log \symLikelihood(
\symNumReg e^{\symPhi_\symSetASuffix},
\symNumReg e^{\symPhi_\symSetBSuffix},
\symNumReg e^{\symPhi_\symSetXSuffix}
\vert
\boldsymbol{\symRegValVariate}_1,
\boldsymbol{\symRegValVariate}_2
)=
\\
\begin{aligned}
&+
\sum_{\symRegVal=1}^{\symRegRange}
\symCountVariate^{<}_{1\symRegVal}
\log\!\left(
\symZ_{\symSetXSuffix\symRegVal}
+
\symY_{\symSetXSuffix\symRegVal}
\symZ_{\symSetASuffix\symRegVal}
\right)
+
\symCountVariate^{<}_{2\symRegVal}
\log\!\left(
\symZ_{\symSetXSuffix\symRegVal}
+
\symY_{\symSetXSuffix\symRegVal}
\symZ_{\symSetBSuffix\symRegVal}
\right)
\\
&+
\sum_{\symRegVal=1}^{\symRegRange}
\symCountVariate^{>}_{1\symRegVal}
\log\!\left(
\symZ_{\symSetASuffix\symRegVal}
\right)
+
\symCountVariate^{>}_{2\symRegVal}
\log\!\left(
\symZ_{\symSetBSuffix\symRegVal}
\right)
+
\symCountVariate^{=}_{\symRegVal}
\log\!\left(
\symZ_{\symSetXSuffix\symRegVal}
+
\symY_{\symSetXSuffix\symRegVal}
\symZ_{\symSetASuffix\symRegVal}
\symZ_{\symSetBSuffix\symRegVal}
\right)
\\
&+
\symCountVariate^{>}_{1,\symRegRange+1}
\log\!\left(
\symZ_{\symSetASuffix\symRegRange}
\right)
+
\symCountVariate^{>}_{2,\symRegRange+1}
\log\!\left(
\symZ_{\symSetBSuffix\symRegRange}
\right)
+
\symCountVariate^{=}_{\symRegRange+1}
\log\!\left(
\symZ_{\symSetXSuffix\symRegRange}
+
\symY_{\symSetXSuffix\symRegRange}
\symZ_{\symSetASuffix\symRegRange}
\symZ_{\symSetBSuffix\symRegRange}
\right)
\\
&-
\sum_{\symRegVal=0}^{\symRegRange}
\left(
  \symCountVariate^{<}_{1\symRegVal}+
  \symCountVariate^{=}_{\symRegVal}+
  \symCountVariate^{>}_{1\symRegVal}
\right)
\symX_{\symSetASuffix\symRegVal}
+
\left(
  \symCountVariate^{<}_{2\symRegVal}+
  \symCountVariate^{=}_{\symRegVal}+
  \symCountVariate^{>}_{2\symRegVal}
\right)
\symX_{\symSetBSuffix\symRegVal}
+
\left(
  \symCountVariate^{<}_{1\symRegVal}+
  \symCountVariate^{=}_{\symRegVal}+
  \symCountVariate^{<}_{2\symRegVal}
\right)
\symX_{\symSetXSuffix\symRegVal}
.
\end{aligned}
\end{multline}
Here we introduced the following expressions for simplification:
\begin{equation}
\begin{aligned}
\symX_{\symSetASuffix\symRegVal} &:=\frac{e^{\symPhi_\symSetASuffix}}{2^{\symRegVal}},
&
\symY_{\symSetASuffix\symRegVal} &:= e^{-\symX_{\symSetASuffix\symRegVal}},
&
\symZ_{\symSetASuffix\symRegVal} &:= 1 - \symY_{\symSetASuffix\symRegVal},
\\
\symX_{\symSetBSuffix\symRegVal} &:=\frac{e^{\symPhi_\symSetBSuffix}}{2^{\symRegVal}},
&
\symY_{\symSetBSuffix\symRegVal} &:= e^{-\symX_{\symSetBSuffix\symRegVal}},
&
\symZ_{\symSetBSuffix\symRegVal} &:= 1 - \symY_{\symSetBSuffix\symRegVal},
\\
\symX_{\symSetXSuffix\symRegVal} &:=\frac{e^{\symPhi_\symSetXSuffix}}{2^{\symRegVal}},
&
\symY_{\symSetXSuffix\symRegVal} &:= e^{-\symX_{\symSetXSuffix\symRegVal}},
&
\symZ_{\symSetXSuffix\symRegVal} &:= 1 - \symY_{\symSetXSuffix\symRegVal}.
\end{aligned}
\end{equation}



Quasi-Newton methods are commonly used for finding the maximum. They require the calculation of the first derivatives which are given by

\begin{multline}
\frac{\partial\symFunc}{\partial\symPhi_\symSetASuffix}
=
\sum_{\symRegVal=1}^{\symRegRange}
\symCountVariate^{<}_{1\symRegVal}
\frac{
\symY_{\symSetXSuffix\symRegVal}
\symX_{\symSetASuffix\symRegVal}
\symY_{\symSetASuffix\symRegVal}
}{
\symZ_{\symSetXSuffix\symRegVal}
+
\symY_{\symSetXSuffix\symRegVal}
\symZ_{\symSetASuffix\symRegVal}
}
+
\symCountVariate^{=}_{\symRegVal}
\frac{
\symY_{\symSetXSuffix\symRegVal}
\symX_{\symSetASuffix\symRegVal}
\symY_{\symSetASuffix\symRegVal}
\symZ_{\symSetBSuffix\symRegVal}
}
{
\symZ_{\symSetXSuffix\symRegVal}
+
\symY_{\symSetXSuffix\symRegVal}
\symZ_{\symSetASuffix\symRegVal}
\symZ_{\symSetBSuffix\symRegVal}
}
+
\symCountVariate^{>}_{1\symRegVal}
\frac{
\symX_{\symSetASuffix\symRegVal}
\symY_{\symSetASuffix\symRegVal}
}{
\symZ_{\symSetASuffix\symRegVal}
}
\\
+
\symCountVariate^{=}_{\symRegRange+1}
\frac{
\symY_{\symSetXSuffix\symRegRange}
\symX_{\symSetASuffix\symRegRange}
\symY_{\symSetASuffix\symRegRange}
\symZ_{\symSetBSuffix\symRegRange}
}
{
\symZ_{\symSetXSuffix\symRegRange}
+
\symY_{\symSetXSuffix\symRegRange}
\symZ_{\symSetASuffix\symRegRange}
\symZ_{\symSetBSuffix\symRegRange}
}
+
\symCountVariate^{>}_{1,\symRegRange+1}
\frac{
\symX_{\symSetASuffix\symRegRange}
\symY_{\symSetASuffix\symRegRange}
}{
\symZ_{\symSetASuffix\symRegRange}
}
-
\sum_{\symRegVal=0}^{\symRegRange}
\left(
  \symCountVariate^{<}_{1\symRegVal}+
  \symCountVariate^{=}_{\symRegVal}+
  \symCountVariate^{>}_{1\symRegVal}
\right)
\symX_{\symSetASuffix\symRegVal}
\end{multline}

\begin{multline}
\frac{\partial\symFunc}{\partial\symPhi_\symSetBSuffix}
=
\sum_{\symRegVal=1}^{\symRegRange}
\symCountVariate^{<}_{1\symRegVal}
\frac{
\symY_{\symSetXSuffix\symRegVal}
\symX_{\symSetBSuffix\symRegVal}
\symY_{\symSetBSuffix\symRegVal}
}{
\symZ_{\symSetXSuffix\symRegVal}
+
\symY_{\symSetXSuffix\symRegVal}
\symZ_{\symSetBSuffix\symRegVal}
}
+
\symCountVariate^{=}_{\symRegVal}
\frac{
\symY_{\symSetXSuffix\symRegVal}
\symX_{\symSetBSuffix\symRegVal}
\symY_{\symSetBSuffix\symRegVal}
\symZ_{\symSetASuffix\symRegVal}
}
{
\symZ_{\symSetXSuffix\symRegVal}
+
\symY_{\symSetXSuffix\symRegVal}
\symZ_{\symSetBSuffix\symRegVal}
\symZ_{\symSetASuffix\symRegVal}
}
+
\symCountVariate^{>}_{1\symRegVal}
\frac{
\symX_{\symSetBSuffix\symRegVal}
\symY_{\symSetBSuffix\symRegVal}
}{
\symZ_{\symSetBSuffix\symRegVal}
}
\\
+
\symCountVariate^{=}_{\symRegRange+1}
\frac{
\symY_{\symSetXSuffix\symRegRange}
\symX_{\symSetBSuffix\symRegRange}
\symY_{\symSetBSuffix\symRegRange}
\symZ_{\symSetASuffix\symRegRange}
}
{
\symZ_{\symSetXSuffix\symRegRange}
+
\symY_{\symSetXSuffix\symRegRange}
\symZ_{\symSetBSuffix\symRegRange}
\symZ_{\symSetASuffix\symRegRange}
}
+
\symCountVariate^{>}_{1,\symRegRange+1}
\frac{
\symX_{\symSetBSuffix\symRegRange}
\symY_{\symSetBSuffix\symRegRange}
}{
\symZ_{\symSetBSuffix\symRegRange}
}
-
\sum_{\symRegVal=0}^{\symRegRange}
\left(
  \symCountVariate^{<}_{1\symRegVal}+
  \symCountVariate^{=}_{\symRegVal}+
  \symCountVariate^{>}_{1\symRegVal}
\right)
\symX_{\symSetBSuffix\symRegVal}
\end{multline}

\begin{multline}
\frac{\partial\symFunc}{\partial\symPhi_\symSetXSuffix}
=
\sum_{\symRegVal=1}^{\symRegRange}
\symCountVariate^{<}_{1\symRegVal}
\frac{
\symY_{\symSetXSuffix\symRegVal}
\symY_{\symSetASuffix\symRegVal}
\symX_{\symSetXSuffix\symRegVal}
}{
\symZ_{\symSetXSuffix\symRegVal}
+
\symY_{\symSetXSuffix\symRegVal}
\symZ_{\symSetASuffix\symRegVal}
}
+
\symCountVariate^{=}_{\symRegVal}
\frac{
\symX_{\symSetXSuffix\symRegVal}
\symY_{\symSetXSuffix\symRegVal}
\left(
\symZ_{\symSetASuffix\symRegVal}
\symY_{\symSetBSuffix\symRegVal}
+
\symY_{\symSetASuffix\symRegVal}
\symZ_{\symSetBSuffix\symRegVal}
+
\symY_{\symSetASuffix\symRegVal}
\symY_{\symSetBSuffix\symRegVal}
\right)
}
{
\symZ_{\symSetXSuffix\symRegVal}
+
\symY_{\symSetXSuffix\symRegVal}
\symZ_{\symSetBSuffix\symRegVal}
\symZ_{\symSetASuffix\symRegVal}
}
+
\symCountVariate^{<}_{2\symRegVal}
\frac{
\symY_{\symSetXSuffix\symRegVal}
\symY_{\symSetBSuffix\symRegVal}
\symX_{\symSetXSuffix\symRegVal}
}{
\symZ_{\symSetXSuffix\symRegVal}
+
\symY_{\symSetXSuffix\symRegVal}
\symZ_{\symSetBSuffix\symRegVal}
}
\\
+
\symCountVariate^{=}_{\symRegRange+1}
\frac{
\symX_{\symSetXSuffix\symRegRange}
\symY_{\symSetXSuffix\symRegRange}
\left(
\symZ_{\symSetASuffix\symRegRange}
\symY_{\symSetBSuffix\symRegRange}
+
\symY_{\symSetASuffix\symRegRange}
\symZ_{\symSetBSuffix\symRegRange}
+
\symY_{\symSetASuffix\symRegRange}
\symY_{\symSetBSuffix\symRegRange}
\right)
}
{
\symZ_{\symSetXSuffix\symRegRange}
+
\symY_{\symSetXSuffix\symRegRange}
\symZ_{\symSetBSuffix\symRegRange}
\symZ_{\symSetASuffix\symRegRange}
}
-
\sum_{\symRegVal=0}^{\symRegRange}
\left(
  \symCountVariate^{<}_{1\symRegVal}+
  \symCountVariate^{=}_{\symRegVal}+
  \symCountVariate^{<}_{2\symRegVal}
\right)
\symX_{\symSetXSuffix\symRegVal}
\end{multline}

\begin{algorithm}
\caption{Joint cardinality estimation.}
\label{alg:joint_cardinality_estimation}
\begin{algorithmic}
\Function {EstimateCardinalities}
{
$
\boldsymbol\symCountVariate^{<}_{1},
\boldsymbol\symCountVariate^{<}_{2},
\boldsymbol\symCountVariate^{=},
\boldsymbol\symCountVariate^{>}_{1},
\boldsymbol\symCountVariate^{>}_{2}
$ 
}
\State $\symNumReg\gets
\left\Vert\boldsymbol\symCountVariate^{<}_{1} + \boldsymbol\symCountVariate^{=} + \boldsymbol\symCountVariate^{>}_{1}\right\Vert_1$
\State $\symPoissonRate_{\symSetASuffix\symSetXSuffix} \gets$ \Call{EstimateCardinality}{$\boldsymbol\symCountVariate^{<}_{1} + \boldsymbol\symCountVariate^{=} + \boldsymbol\symCountVariate^{>}_{1}$}
\State $\symPoissonRate_{\symSetBSuffix\symSetXSuffix} \gets$ \Call{EstimateCardinality}{$\boldsymbol\symCountVariate^{<}_{2} + \boldsymbol\symCountVariate^{=} + \boldsymbol\symCountVariate^{>}_{2}$}
\If{$\symCountVariate^{<}_{10}+\symCountVariate^{=}_0+\symCountVariate^{<}_{20} = \symNumReg$}
\State \textbf{return} $\left(\symPoissonRate_{\symSetASuffix\symSetXSuffix},\symPoissonRate_{\symSetBSuffix\symSetXSuffix},0\right)$
\EndIf
\State $\symPoissonRate_{\symSetASuffix\symSetBSuffix\symSetXSuffix} \gets$ \Call{EstimateCardinality}{$\boldsymbol\symCountVariate^{>}_{1} + \boldsymbol\symCountVariate^{=} + \boldsymbol\symCountVariate^{>}_{2}$}
\State $\symPhi_\symSetASuffix = \log(\max(1, \symPoissonRate_{\symSetASuffix\symSetBSuffix\symSetXSuffix} - \symPoissonRate_{\symSetBSuffix\symSetXSuffix})/\symNumReg)$
\State $\symPhi_\symSetBSuffix = \log(\max(1, \symPoissonRate_{\symSetASuffix\symSetBSuffix\symSetXSuffix} - \symPoissonRate_{\symSetASuffix\symSetXSuffix})/\symNumReg)$
\State $\symPhi_\symSetXSuffix = \log(\max(1,
\symPoissonRate_{\symSetASuffix\symSetXSuffix}+
\symPoissonRate_{\symSetBSuffix\symSetXSuffix}
-
\symPoissonRate_{\symSetASuffix\symSetBSuffix\symSetXSuffix})/\symNumReg)$
\Repeat
\State $\symPhi_\symSetASuffix'\gets\symPhi_\symSetASuffix,\ \symPhi'_\symSetBSuffix\gets\symPhi_\symSetBSuffix,\  \symPhi'_\symSetXSuffix\gets\symPhi_\symSetXSuffix$
\State do BFGS algorithm step 
\Until{
$
\left|\symPhi_\symSetASuffix - \symPhi'_\symSetASuffix\right| < \symError
\wedge
\left|\symPhi_\symSetBSuffix - \symPhi'_\symSetBSuffix\right| < \symError
\wedge
\left|\symPhi_\symSetXSuffix - \symPhi'_\symSetXSuffix\right| < \symError
$}
\State \textbf{return} $\left(
\symNumReg e^{\symPhi_\symSetASuffix},
\symNumReg e^{\symPhi_\symSetBSuffix},
\symNumReg e^{\symPhi_\symSetXSuffix}
\right)$
\EndFunction
\end{algorithmic}
\end{algorithm}


\subsection{Results}

TODO see \cref{tbl:joint_estimation_cases}, \cref{tbl:joint_estimation_a}, \cref{tbl:joint_estimation_b}, \cref{tbl:joint_estimation_x}


\begin{table}
\centering
\caption{The list of cases for which the cardinalities $|\symSetA|$, $|\symSetB|$, and $|\symSetX|$ have been estimated given two HyperLogLog sketches with parameters $\symPrecision=20$ and $\symRegRange=44$ representing $\symSetA\cup\symSetX$ and  $\symSetB\cup\symSetX$, respectively.}
\label{tbl:joint_estimation_cases}
\csvreader[before reading=\footnotesize, tabular=r|r|r|r|r|r, table head=\# & $|\symSetA|$ & $|\symSetB|$ & $|\symSetX|$ & $\frac{|\symSetX|}{|\symSetA\cup\symSetB\cup\symSetX|}$ & $\log_{10}\!\left(\frac{|\symSetA\cup\symSetX|}{|\symSetB\cup\symSetX|}\right)$\\\hline]
{intersection.csv}{trueCardA=\trueCardA,trueCardB=\trueCardB,trueCardX=\trueCardX,trueJaccardIdx=\trueJaccardIdx ,trueLogRatio=\trueLogRatio}{
\thecsvrow & 
\num{\trueCardA} & 
\num{\trueCardB} & 
\num{\trueCardX} & 
\numformat{\trueJaccardIdx} & 
\numformattwo{\trueLogRatio}
}
\end{table}

\begin{table}
\centering
\caption{The mean and the standard deviation for the cases given in \cref{tbl:joint_estimation_cases} when estimating $|\symSetA|$ using the inclusion-exlusion principle and the maximum likelihood method, respectively.}
\label{tbl:joint_estimation_a}
\csvreader[before reading=\footnotesize, tabular=r||r|r|r||r|r|r||r, table head=
 & 
\multicolumn{3}{c||}{inclusion-exclusion principle}
&
\multicolumn{3}{c||}{maximum likelihood method}
&
\multicolumn{1}{c}{improvement}
\\
\# & 
\multicolumn{1}{c|}{mean} & 
\multicolumn{1}{c|}{stdev} & 
\multicolumn{1}{c||}{rmse} & 
\multicolumn{1}{c|}{mean} & 
\multicolumn{1}{c|}{stdev} & 
\multicolumn{1}{c||}{rmse} &
\multicolumn{1}{c}{rmse ratio}
\\\hline]
{intersection.csv}{
inclExclMeanA=\inclExclMeanA,
inclExclStdDevA=\inclExclStdDevA,
inclExclRMSEA=\inclExclRMSEA,
maxLikeMeanA=\maxLikeMeanA,
maxLikeStdDevA=\maxLikeStdDevA,
maxLikeRMSEA=\maxLikeRMSEA,
improvementRmseA=\improvementRmseA
}{
\thecsvrow & 
\numformat{\inclExclMeanA} & 
\numformat{\inclExclStdDevA} & 
\numformat{\inclExclRMSEA} & 
\numformat{\maxLikeMeanA} & 
\numformat{\maxLikeStdDevA} & 
\numformat{\maxLikeRMSEA} &
\numformattwo{\improvementRmseA}
}
\end{table}


\begin{table}
\centering
\caption{The mean and the standard deviation for the cases given in \cref{tbl:joint_estimation_cases} when estimating $|\symSetB|$ using the inclusion-exlusion principle and the maximum likelihood method, respectively.}
\label{tbl:joint_estimation_b}
\csvreader[before reading=\footnotesize, tabular=r||r|r|r||r|r|r||r, table head=
 & 
\multicolumn{3}{c||}{inclusion-exclusion principle}
&
\multicolumn{3}{c||}{maximum likelihood method}
&
\multicolumn{1}{c}{improvement}
\\
\# & 
\multicolumn{1}{c|}{mean} & 
\multicolumn{1}{c|}{stdev} & 
\multicolumn{1}{c||}{rmse} & 
\multicolumn{1}{c|}{mean} & 
\multicolumn{1}{c|}{stdev} & 
\multicolumn{1}{c||}{rmse} &
\multicolumn{1}{c}{rmse ratio}
\\\hline]
{intersection.csv}{
inclExclMeanB=\inclExclMeanB,
inclExclStdDevB=\inclExclStdDevB,
inclExclRMSEB=\inclExclRMSEB,
maxLikeMeanB=\maxLikeMeanB,
maxLikeStdDevB=\maxLikeStdDevB,
maxLikeRMSEB=\maxLikeRMSEB,
improvementRmseB=\improvementRmseB
}{
\thecsvrow & 
\numformat{\inclExclMeanB} & 
\numformat{\inclExclStdDevB} & 
\numformat{\inclExclRMSEB} & 
\numformat{\maxLikeMeanB} & 
\numformat{\maxLikeStdDevB} & 
\numformat{\maxLikeRMSEB} &
\numformattwo{\improvementRmseB}
}
\end{table}

\begin{table}
\centering
\caption{The mean and the standard deviation for the cases given in \cref{tbl:joint_estimation_cases} when estimating $|\symSetX|$ using the inclusion-exlusion principle and the maximum likelihood method, respectively.}
\label{tbl:joint_estimation_x}
\csvreader[before reading=\footnotesize, tabular=r||r|r|r||r|r|r||r, table head=
 & 
\multicolumn{3}{c||}{inclusion-exclusion principle}
&
\multicolumn{3}{c||}{maximum likelihood method}
&
\multicolumn{1}{c}{improvement}
\\
\# & 
\multicolumn{1}{c|}{mean} & 
\multicolumn{1}{c|}{stdev} & 
\multicolumn{1}{c||}{rmse} & 
\multicolumn{1}{c|}{mean} & 
\multicolumn{1}{c|}{stdev} & 
\multicolumn{1}{c||}{rmse} &
\multicolumn{1}{c}{rmse ratio}
\\\hline]
{intersection.csv}{
inclExclMeanX=\inclExclMeanX,
inclExclStdDevX=\inclExclStdDevX,
inclExclRMSEX=\inclExclRMSEX,
maxLikeMeanX=\maxLikeMeanX,
maxLikeStdDevX=\maxLikeStdDevX,
maxLikeRMSEX=\maxLikeRMSEX,
improvementRmseX=\improvementRmseX
}{
\thecsvrow & 
\numformat{\inclExclMeanX} & 
\numformat{\inclExclStdDevX} & 
\numformat{\inclExclRMSEX} & 
\numformat{\maxLikeMeanX} & 
\numformat{\maxLikeStdDevX} & 
\numformat{\maxLikeRMSEX} &
\numformattwo{\improvementRmseX}
}
\end{table}

\section{Conclusion and future work}

TODO

\appendix

\section{Numerical stability of recursion formula for $\symHelper(\symX)$}
\label{app:helper_stable}
In order to investigate the error propagation of a single recursion step using \eqref{equ:helper_recursion1} we define $\symHelper_1 := \symHelper(2\symX)$ and $\symHelper_2 := \symHelper(4\symX)$. The recursion formula simplifies to
\begin{equation}
\label{equ:h2}
\symHelper_2 = \frac{\symX+\symHelper_1(1-\symHelper_1)}{\symX+(1-\symHelper_1)}.
\end{equation}
If $\symHelper_1$ is approximated by $\tilde{\symHelper}_1 = \symHelper_1\left(1+\symError_1\right)$ with relative error $\symError_1$, the recursion formula will give an approximation for $\symHelper_2$
\begin{equation}
\label{equ:h2_approx}
\tilde{\symHelper}_2 = 
\frac{\symX+\tilde{\symHelper}_1(1-\tilde{\symHelper}_1)}{\symX+(1-\tilde{\symHelper}_1)}
\end{equation}
The corresponding relative error $\symError_2$ is given by
\begin{equation}
\label{equ:h2_relative_error}
\symError_2 = \frac{\tilde{\symHelper}_2}{\symHelper_2}-1.
\end{equation}
Combination of \eqref{equ:h2}, \eqref{equ:h2_approx}, and \eqref{equ:h2_relative_error} yields for its absolute value
\begin{equation}
\left|\symError_2\right|
=
\left|\symError_1\right|
\frac{
\left|
\frac{\symHelper_1\left(1-2\symHelper_1\right)}{\symX+\symHelper_1\left(1-\symHelper_1\right)}
+
\frac{\symHelper_1}{\symX+1-\symHelper_1}
-
\symError_1
\frac{\symHelper_1^2}{\symX+\symHelper_1\left(1-\symHelper_1\right)}
\right|
}{
\left|
1-
\symError_1
\frac{\symHelper_1}{\symX+1-\symHelper_1}
\right|
}.
\end{equation}
The triangle inequality leads to
\begin{equation}
\left|\symError_2\right|
\leq
\left|\symError_1\right|
\frac{
\left|
\frac{\symHelper_1\left(1-2\symHelper_1\right)}{\symX+\symHelper_1\left(1-\symHelper_1\right)}
+
\frac{\symHelper_1}{\symX+1-\symHelper_1}
\right|
+
\left|
\symError_1
\right|
\frac{\symHelper_1^2}{\symX+\symHelper_1\left(1-\symHelper_1\right)}
}{
\left|
1-
\symError_1
\frac{\symHelper_1}{\symX+1-\symHelper_1}
\right|
}
\end{equation}

By numerical means it is easy to show that the inequalities $\left|
\frac{\symHelper_1\left(1-2\symHelper_1\right)}{\symX+\symHelper_1\left(1-\symHelper_1\right)}
+
\frac{\symHelper_1}{\symX+1-\symHelper_1}
\right|\leq\input{max_errror_propagation_factor1.txt}
$,
$\frac{\symHelper_1^2}{\symX+\symHelper_1\left(1-\symHelper_1\right)}
\leq\input{max_errror_propagation_factor2.txt}$,
and
$\frac{\symHelper_1}{\symX+1-\symHelper_1}\leq\input{max_errror_propagation_factor3.txt}$ hold for all $\symX\geq0$. Therefore, if we additionally assume, for example, $\left|\symError_1\right|\leq0.1$, we get
\begin{equation}
\left|\symError_2\right|
\leq
\left|\symError_1\right|
\frac{\input{max_errror_propagation_factor1.txt} + 0.1\cdot\input{max_errror_propagation_factor2.txt}}{1-0.1\cdot\input{max_errror_propagation_factor3.txt}}
\leq
\left|\symError_1\right|
\cdot
\input{max_errror_propagation_factor4.txt},
\end{equation}
which means that the relative error is decreasing in each recursion step and the recursive calculation of $\symHelper$ is numerically stable.

\section{Error caused by approximation of $\symHelper(\symX)$}
\label{app:error_approx}
According to \eqref{equ:func} the exact estimate $\symXEstimate$ fulfills 
\begin{equation}
\label{equ:max_likelihood_2}
\symXEstimate\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}+
\sum_{\symRegVal=1}^\symRegRange \symCountVariate_\symRegVal\symHelper\!\left(\frac{\symXEstimate}{2^\symRegVal}\right)
+
\symCountVariate_{\symRegRange+1}\symHelper\!\left(\frac{\symXEstimate}{2^\symRegRange}\right)
=
\symNumReg-\symCountVariate_0
\end{equation}
If $\symHelper$ is not calculated exactly but approximated by $\symHelperApprox$ with maximum relative error $\symError_\symHelper\ll 1$
\begin{equation}
\label{equ:abs_error_helper}
\left|\symHelperApprox(\symX) - \symHelper(\symX) \right|  \leq \symError_\symHelper\symHelper(\symX)
\end{equation}
the solution of the equation will be off by a relative error $\symError_\symX$:
\begin{equation}
\label{equ:ml_equation}
\symXEstimate\left(1+\symError_\symX\right)\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}+
\sum_{\symRegVal=1}^\symRegRange \symCountVariate_\symRegVal\symHelperApprox\!\left(\frac{\symXEstimate\left(1+\symError_\symX\right)}{2^\symRegVal}\right)
+
\symCountVariate_{\symRegRange+1}
\symHelperApprox\!\left(
\frac{\symXEstimate\left(1+\symError_\symX\right)}{2^\symRegRange}\right)
=
\symNumReg-\symCountVariate_0
\end{equation}
Due to \eqref{equ:abs_error_helper} there exists some $\symAlpha \in [-\symError_\symHelper, \symError_\symHelper]$ for which
\begin{multline}
\symXEstimate\left(1+\symError_\symX\right)\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}+
\left(
1+\symAlpha
\right)
\left(
\sum_{\symRegVal=1}^\symRegRange \symCountVariate_\symRegVal
\symHelper\!\left(\frac{\symXEstimate\left(1+\symError_\symX\right)
}{2^\symRegVal}\right)
+
\symCountVariate_{\symRegRange+1}
\symHelper\!\left(
\frac{\symXEstimate\left(1+\symError_\symX\right)}{2^\symRegRange}\right)
\right)
=
\symNumReg-\symCountVariate_0
\end{multline}
Since $\symHelper'(\symX)\in[0,0.5]$ for $\symX \geq 0$, there exists a $\symBeta\in[0,0.5]$ for which
\begin{multline}
\label{equ:appendix3}
\symXEstimate\left(1+\symError_\symX\right)\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}+
\left(
1+\symAlpha
\right)
\left(
\sum_{\symRegVal=1}^\symRegRange 
\symCountVariate_\symRegVal
\symHelper\!\left(\frac{\symXEstimate
}{2^\symRegVal}\right)
+
\frac{\symCountVariate_\symRegVal}{2^\symRegVal}
\symXEstimate\symError_\symX
\symBeta
\right)
+
\\
+
\left(
1+\symAlpha
\right)
\left(
\symCountVariate_{\symRegRange+1}
\symHelper\!\left(
\frac{\symXEstimate}{2^\symRegRange}\right)
+
\frac{\symCountVariate_{\symRegRange+1}}{2^\symRegRange}
\symXEstimate\symError_\symX\symBeta
\right)
=
\symNumReg-\symCountVariate_0
\end{multline}
Subtracting \eqref{equ:max_likelihood_2} multiplied by $\left(1+\symAlpha\right)$ from \eqref{equ:appendix3} and resolving 
$\symError_\symX$ gives
\begin{equation}
\symError_\symX
=
\symAlpha
\frac{
\symXEstimate
\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}
-\left(\symNumReg-\symCountVariate_0\right)
}
{
\symXEstimate
\left(
\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}
+
\left(1+\symAlpha\right)
\symBeta
\left(
\sum_{\symRegVal=1}^\symRegRange 
\frac{\symCountVariate_\symRegVal
}{2^\symRegVal}
+
\frac{\symCountVariate_{\symRegRange+1}
}{2^\symRegRange}
\right)
\right)
}
\end{equation}
Using $\left|\symAlpha\right|\leq\symError_\symHelper$, $\symBeta\geq0$, and \eqref{equ:weak_upper_bound} the absolute value of the relative error
can be bounded by
\begin{equation}
\left|\symError_\symX\right| 
\leq
\left|\symError_\symHelper\right| 
\frac{
\left(\symNumReg-\symCountVariate_0\right)-
\symXEstimate
\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}
}
{
\symXEstimate
\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}
}
\end{equation}
Furthermore, using \eqref{equ:weak_lower_bound} we finally get
\begin{equation}
\left|\symError_\symX\right| 
\leq
\left|\symError_\symHelper\right|
\frac{
\frac{1}{2}
\sum_{\symRegVal=1}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}
+
\frac{\symCountVariate_{\symRegRange+1}
}{2^\symRegRange}
}
{
\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}
}
\leq
\left|\symError_\symHelper\right|
\left(
\frac{1}{2}
+
\frac{
\frac{\symCountVariate_{\symRegRange+1}
}{2^\symRegRange}
}
{
\sum_{\symRegVal=0}^\symRegRange \frac{\symCountVariate_\symRegVal}{2^\symRegVal}
}
\right)
\leq
\left|\symError_\symHelper\right|
\left(
\frac{1}{2}
+
\frac{
\symCountVariate_{\symRegRange+1}
}
{
\symNumReg-\symCountVariate_{\symRegRange+1}
}
\right)
\end{equation}
Hence, as long as most registers are not in the saturated state ($\symCountVariate_{\symRegRange+1}\ll\symNumReg$), the relative error $\symError_\symX$ of the calculated estimate using the approximation $\symHelperApprox(\symX)$ for $\symHelper(\symX)$ has the same order of magnitude as $\symError_\symHelper$.

\bibliographystyle{unsrt}
\bibliography{bibliography.bib}

\end{document}



 

 
